{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/timoklein/Library/CloudStorage/OneDrive-Personal/Universiteit/Studie/M_Data_Science_and_Society/year_2/Thesis/Code/timoklein\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Technology Readiness Index dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country not found: Bolivia (Plurinational State of)\n",
      "Country not found: China, Hong Kong SAR\n",
      "Country not found: Congo, Dem. Rep. of the\n",
      "Country not found: Cote d'Ivoire\n",
      "Country not found: Iran (Islamic Republic of)\n",
      "Country not found: Lao People's Dem. Rep.\n",
      "Country not found: Netherlands (Kingdom of the)\n",
      "Country not found: Switzerland, Liechtenstein\n",
      "Country not found: Turkiye\n",
      "Country not found: Venezuela (Bolivarian Rep. of)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Finance_access</th>\n",
       "      <th>ICT</th>\n",
       "      <th>Industry_activity</th>\n",
       "      <th>Overall_index</th>\n",
       "      <th>Research_and_development</th>\n",
       "      <th>Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALB</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DZA</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country  Finance_access   ICT  Industry_activity  Overall_index  \\\n",
       "0     AFG            0.15  0.00               0.25           0.00   \n",
       "1     ALB            0.60  0.50               0.50           0.40   \n",
       "2     DZA            0.50  0.25               0.20           0.30   \n",
       "3     AGO            0.50  0.10               0.25           0.15   \n",
       "4     ARG            0.40  0.55               0.60           0.55   \n",
       "\n",
       "   Research_and_development  Skills  \n",
       "0                      0.00    0.10  \n",
       "1                      0.10    0.45  \n",
       "2                      0.30    0.40  \n",
       "3                      0.05    0.20  \n",
       "4                      0.30    0.65  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the Technology Readiness Index dataset into a DataFrame\n",
    "df_tri = pd.read_csv('Data/Technology_Readiness_Index.csv')\n",
    "\n",
    "# Only keep the columns 'Economy_Label', 'Category_Label', '2015_Index_Value' and '2016_Index_Value'\n",
    "df_tri = df_tri[['Economy_Label', 'Category_Label', '2015_Index_Value', '2016_Index_Value']]\n",
    "\n",
    "# Add a column with the average index value for 2015 and 2016\n",
    "df_tri['Average_Index_Value'] = df_tri[['2015_Index_Value', '2016_Index_Value']].mean(axis=1)\n",
    "\n",
    "# Delete the 2015 and 2016 index value columns\n",
    "df_tri.drop(columns=['2015_Index_Value', '2016_Index_Value'], inplace=True)\n",
    "\n",
    "# Pivot the dataframe\n",
    "df_tri_cleaned = df_tri.pivot(index='Economy_Label', columns='Category_Label', values=['Average_Index_Value'])\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "df_tri_cleaned.columns = ['_'.join(col).strip() for col in df_tri_cleaned.columns.values]\n",
    "\n",
    "# Delete the row with 'Economy_Label'\n",
    "df_tri_cleaned.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "df_tri_cleaned.columns = ['Country', 'Finance_access', 'ICT', 'Industry_activity', 'Overall_index', 'Research_and_development', 'Skills']\n",
    "\n",
    "# Drop missing values\n",
    "df_tri_cleaned.dropna(inplace=True)\n",
    "\n",
    "# Change the 'Country' column to ISO codes\n",
    "# Extract the 'Country' column as a list\n",
    "country_names = df_tri_cleaned['Country'].tolist()\n",
    "\n",
    "# Create a mapping of country names to ISO codes\n",
    "country_to_iso = {}\n",
    "for country in country_names:\n",
    "    try:\n",
    "        # Get the country object using the name\n",
    "        country_obj = pycountry.countries.lookup(country)\n",
    "        # Map country name to ISO code\n",
    "        country_to_iso[country] = country_obj.alpha_3  # Using 3-letter ISO code\n",
    "    except LookupError:\n",
    "        # If the country is not found, handle it as needed\n",
    "        print(f\"Country not found: {country}\")\n",
    "\n",
    "# Manual mapping for countries that did not get an ISO code\n",
    "manual_iso_mapping = {\n",
    "    \"Bolivia (Plurinational State of)\": \"BOL\",\n",
    "    \"China, Hong Kong SAR\": \"HKG\",\n",
    "    \"Congo, Dem. Rep. of the\": \"COD\",\n",
    "    \"Cote d'Ivoire\": \"CIV\",\n",
    "    \"Iran (Islamic Republic of)\": \"IRN\",\n",
    "    \"Lao People's Dem. Rep.\": \"LAO\",\n",
    "    \"Netherlands (Kingdom of the)\": \"NLD\",\n",
    "    \"Switzerland, Liechtenstein\": \"CHE\",  \n",
    "    \"Turkiye\": \"TUR\",  \n",
    "    \"Venezuela (Bolivarian Rep. of)\": \"VEN\"\n",
    "}\n",
    "\n",
    "# Update the mapping with the manual mappings\n",
    "country_to_iso.update(manual_iso_mapping)\n",
    "\n",
    "# Create a Series from the mapping to use for replacing values\n",
    "iso_codes_series = pd.Series(country_to_iso)\n",
    "\n",
    "# Replace the 'Country' column values with ISO codes\n",
    "df_tri_cleaned['Country'] = df_tri_cleaned['Country'].replace(iso_codes_series)\n",
    "\n",
    "df_tri_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Finance_access</th>\n",
       "      <th>ICT</th>\n",
       "      <th>Industry_activity</th>\n",
       "      <th>Overall_index</th>\n",
       "      <th>Research_and_development</th>\n",
       "      <th>Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.628614</td>\n",
       "      <td>0.423494</td>\n",
       "      <td>0.574699</td>\n",
       "      <td>0.450904</td>\n",
       "      <td>0.246084</td>\n",
       "      <td>0.432229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.180766</td>\n",
       "      <td>0.249768</td>\n",
       "      <td>0.193199</td>\n",
       "      <td>0.262648</td>\n",
       "      <td>0.240580</td>\n",
       "      <td>0.238875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Finance_access         ICT  Industry_activity  Overall_index  \\\n",
       "count      166.000000  166.000000         166.000000     166.000000   \n",
       "mean         0.628614    0.423494           0.574699       0.450904   \n",
       "std          0.180766    0.249768           0.193199       0.262648   \n",
       "min          0.000000    0.000000           0.000000       0.000000   \n",
       "25%          0.500000    0.200000           0.400000       0.200000   \n",
       "50%          0.650000    0.450000           0.600000       0.400000   \n",
       "75%          0.750000    0.600000           0.700000       0.650000   \n",
       "max          1.000000    1.000000           1.000000       1.000000   \n",
       "\n",
       "       Research_and_development      Skills  \n",
       "count                166.000000  166.000000  \n",
       "mean                   0.246084    0.432229  \n",
       "std                    0.240580    0.238875  \n",
       "min                    0.000000    0.000000  \n",
       "25%                    0.050000    0.212500  \n",
       "50%                    0.150000    0.400000  \n",
       "75%                    0.400000    0.600000  \n",
       "max                    1.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tri_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Road Traffic Deaths Sex dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country not found: Bolivia (Plurinational State of)\n",
      "Country not found: Cote d'Ivoire\n",
      "Country not found: Democratic Republic of the Congo\n",
      "Country not found: Iran (Islamic Republic of)\n",
      "Country not found: Micronesia (Federated States of)\n",
      "Country not found: Netherlands (Kingdom of the)\n",
      "Country not found: Republic of Korea\n",
      "Country not found: Turkiye\n",
      "Country not found: Venezuela (Bolivarian Republic of)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>14.60</td>\n",
       "      <td>22.90</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALB</td>\n",
       "      <td>13.80</td>\n",
       "      <td>20.90</td>\n",
       "      <td>6.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DZA</td>\n",
       "      <td>21.15</td>\n",
       "      <td>28.40</td>\n",
       "      <td>13.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO</td>\n",
       "      <td>24.15</td>\n",
       "      <td>30.80</td>\n",
       "      <td>17.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATG</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARG</td>\n",
       "      <td>14.00</td>\n",
       "      <td>22.35</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ARM</td>\n",
       "      <td>16.20</td>\n",
       "      <td>25.10</td>\n",
       "      <td>8.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AUS</td>\n",
       "      <td>5.40</td>\n",
       "      <td>7.80</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AUT</td>\n",
       "      <td>5.45</td>\n",
       "      <td>7.95</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AZE</td>\n",
       "      <td>9.50</td>\n",
       "      <td>15.25</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country  Total  Males  Females\n",
       "0     AFG  14.60  22.90     5.75\n",
       "1     ALB  13.80  20.90     6.45\n",
       "2     DZA  21.15  28.40    13.70\n",
       "3     AGO  24.15  30.80    17.75\n",
       "4     ATG   0.00   0.00     0.00\n",
       "5     ARG  14.00  22.35     6.00\n",
       "6     ARM  16.20  25.10     8.35\n",
       "7     AUS   5.40   7.80     2.95\n",
       "8     AUT   5.45   7.95     3.00\n",
       "9     AZE   9.50  15.25     3.80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the Road Traffic Deaths Sex dataset into a DataFrame\n",
    "df_rtds = pd.read_csv('Data/Road_Traffic_Deaths_Sex.csv')\n",
    "\n",
    "# Only keep the columns 'Unnamed: 0', 'Unnamed: 1', 'Estimated road traffic death rate (per 100 000 population)', 'Estimated road traffic death rate (per 100 000 population).1' and 'Estimated road traffic death rate (per 100 000 population).2'\n",
    "df_rtds = df_rtds[['Unnamed: 0', 'Unnamed: 1', 'Estimated road traffic death rate (per 100 000 population)', 'Estimated road traffic death rate (per 100 000 population).1', 'Estimated road traffic death rate (per 100 000 population).2']]\n",
    "\n",
    "# Rename the columns and delete the first row\n",
    "df_rtds.columns = ['Country', 'Year', 'Total', 'Males', 'Females']\n",
    "df_rtds = df_rtds.iloc[1:]\n",
    "\n",
    "# Only keep the rows where the 'Year' column is '2015' or '2016'\n",
    "df_rtds['Year'] = df_rtds['Year'].str.strip()\n",
    "df_rtds = df_rtds[df_rtds['Year'].isin(['2015', '2016'])]\n",
    "\n",
    "# Remove the numbers between square brackets\n",
    "columns_to_clean = ['Total','Males','Females']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    df_rtds[col] = df_rtds[col].str.replace(r'\\s*\\[.*?\\]', '', regex=True)\n",
    "\n",
    "# Convert the death rate columns to numeric\n",
    "df_rtds['Total'] = pd.to_numeric(df_rtds['Total'], errors='coerce')\n",
    "df_rtds['Males'] = pd.to_numeric(df_rtds['Males'], errors='coerce')\n",
    "df_rtds['Females'] = pd.to_numeric(df_rtds['Females'], errors='coerce')\n",
    "df_rtds['Year'] = pd.to_numeric(df_rtds['Year'], errors='coerce')\n",
    "\n",
    "# Group by 'Country' and calculate the mean for the last three columns for '2015' and '2016'\n",
    "df_rtds = df_rtds.groupby('Country').mean()[['Total','Males','Females']]\n",
    "\n",
    "# Normalize the last three columns \n",
    "# Convert columns to numeric type\n",
    "columns_to_normalize = ['Total','Males','Females']\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    df_rtds[col] = pd.to_numeric(df_rtds[col])\n",
    "\n",
    "# Min-Max Normalization directly in the existing DataFrame\n",
    "#for col in columns_to_normalize:\n",
    "#    df_rtds[col] = (df_rtds[col] - df_rtds[col].min()) / (df_rtds[col].max() - df_rtds[col].min())\n",
    "\n",
    "# Reset the index so that \"Country\" becomes a regular column\n",
    "df_rtds = df_rtds.reset_index()\n",
    "\n",
    "# Change the 'Country' column to ISO codes\n",
    "# Extract the 'Country' column as a list\n",
    "country_names = df_rtds['Country'].tolist()\n",
    "\n",
    "# Create a mapping of country names to ISO codes\n",
    "country_to_iso = {}\n",
    "for country in country_names:\n",
    "    try:\n",
    "        # Get the country object using the name\n",
    "        country_obj = pycountry.countries.lookup(country)\n",
    "        # Map country name to ISO code\n",
    "        country_to_iso[country] = country_obj.alpha_3  # Using 3-letter ISO code\n",
    "    except LookupError:\n",
    "        # If the country is not found, handle it as needed\n",
    "        print(f\"Country not found: {country}\")\n",
    "\n",
    "# Manual mapping for countries that did not get an ISO code\n",
    "manual_iso_mapping = {\n",
    "    \"Bolivia (Plurinational State of)\": \"BOL\",\n",
    "    \"Cote d'Ivoire\": \"CIV\",  \n",
    "    \"Democratic Republic of the Congo\": \"COD\",\n",
    "    \"Iran (Islamic Republic of)\": \"IRN\",\n",
    "    \"Micronesia (Federated States of)\": \"FSM\",\n",
    "    \"Netherlands (Kingdom of the)\": \"NLD\",\n",
    "    \"Republic of Korea\": \"KOR\",\n",
    "    \"Turkiye\": \"TUR\",  \n",
    "    \"Venezuela (Bolivarian Republic of)\": \"VEN\"\n",
    "}\n",
    "\n",
    "# Update the mapping with the manual mappings\n",
    "country_to_iso.update(manual_iso_mapping)\n",
    "\n",
    "# Create a Series from the mapping to use for replacing values\n",
    "iso_codes_series = pd.Series(country_to_iso)\n",
    "\n",
    "# Replace the 'Country' column values with ISO codes\n",
    "df_rtds['Country'] = df_rtds['Country'].replace(iso_codes_series)\n",
    "\n",
    "df_rtds.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.063934</td>\n",
       "      <td>25.370765</td>\n",
       "      <td>8.760383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.388973</td>\n",
       "      <td>13.848550</td>\n",
       "      <td>5.802119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>4.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.050000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>7.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.775000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>13.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.300000</td>\n",
       "      <td>64.400000</td>\n",
       "      <td>23.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total       Males     Females\n",
       "count  183.000000  183.000000  183.000000\n",
       "mean    17.063934   25.370765    8.760383\n",
       "std      9.388973   13.848550    5.802119\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      9.500000   14.450000    4.175000\n",
       "50%     16.050000   24.250000    7.250000\n",
       "75%     24.775000   35.500000   13.025000\n",
       "max     39.300000   64.400000   23.900000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rtds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Road Traffic Deaths User dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country not found: Bolivia (Plurinational State of)\n",
      "Country not found: Cote d'Ivoire\n",
      "Country not found: Democratic Republic of the Congo\n",
      "Country not found: Iran (Islamic Republic of)\n",
      "Country not found: Micronesia (Federated States of)\n",
      "Country not found: Netherlands (Kingdom of the)\n",
      "Country not found: Republic of Korea\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Pedestrians</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>THA</td>\n",
       "      <td>15.1450</td>\n",
       "      <td>2.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>TON</td>\n",
       "      <td>10.2884</td>\n",
       "      <td>4.9484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>TTO</td>\n",
       "      <td>5.9160</td>\n",
       "      <td>3.7320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>TUN</td>\n",
       "      <td>4.9910</td>\n",
       "      <td>5.6580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>UGA</td>\n",
       "      <td>10.3360</td>\n",
       "      <td>12.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>UKR</td>\n",
       "      <td>7.4120</td>\n",
       "      <td>5.6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ARE</td>\n",
       "      <td>8.2698</td>\n",
       "      <td>4.3497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>GBR</td>\n",
       "      <td>1.1550</td>\n",
       "      <td>0.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>TZA</td>\n",
       "      <td>19.5534</td>\n",
       "      <td>9.1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>USA</td>\n",
       "      <td>3.9603</td>\n",
       "      <td>1.9737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country  Passengers  Pedestrians\n",
       "107     THA     15.1450       2.4700\n",
       "108     TON     10.2884       4.9484\n",
       "109     TTO      5.9160       3.7320\n",
       "110     TUN      4.9910       5.6580\n",
       "111     UGA     10.3360      12.0080\n",
       "112     UKR      7.4120       5.6984\n",
       "113     ARE      8.2698       4.3497\n",
       "114     GBR      1.1550       0.7110\n",
       "115     TZA     19.5534       9.1494\n",
       "116     USA      3.9603       1.9737"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the Road Traffic Deaths Sex dataset into a DataFrame\n",
    "df_rtdsfu = pd.read_csv('Data/Road_Traffic_Deaths_Sex.csv')\n",
    "\n",
    "## Read the Road Traffic Deaths User dataset into a DataFrame\n",
    "df_rtdu = pd.read_csv('Data/Road_Traffic_Deaths_User.csv')\n",
    "\n",
    "# Only keep the columns 'Unnamed: 0' and 'Estimated number of road traffic deaths'\n",
    "df_rtdsfu = df_rtdsfu[['Unnamed: 0','Unnamed: 1','Estimated road traffic death rate (per 100 000 population)']]\n",
    "\n",
    "# Rename the columns and delete the first row\n",
    "df_rtdsfu.columns = ['Country', 'Year','Total']\n",
    "df_rtdsfu = df_rtdsfu.iloc[1:]\n",
    "\n",
    "# Only keep the rows where the 'Year' column is '2016'\n",
    "df_rtdsfu['Year'] = df_rtdsfu['Year'].str.strip()\n",
    "df_rtdsfu = df_rtdsfu[df_rtdsfu['Year'].isin(['2016'])]\n",
    "\n",
    "# Remove the numbers between square brackets\n",
    "df_rtdsfu['Total'] = df_rtdsfu['Total'].str.replace(r'\\s*\\[.*?\\]', '', regex=True)\n",
    "\n",
    "# Delete the 'Year' column\n",
    "df_rtdsfu.drop(columns=['Year'], inplace=True)\n",
    "\n",
    "# Only keep the columns 'Unnamed: 0','Unnamed: 1', 'Distribution of road traffic deaths by type of road user (%)' and 'Distribution of road traffic deaths by type of road user (%).3'\n",
    "df_rtdu = df_rtdu[['Unnamed: 0','Unnamed: 1','Distribution of road traffic deaths by type of road user (%)', 'Distribution of road traffic deaths by type of road user (%).3']]\n",
    "\n",
    "# Rename the columns and delete the first row\n",
    "df_rtdu.columns = ['Country', 'Year','Passengers', 'Pedestrians']\n",
    "df_rtdu = df_rtdu.iloc[1:]\n",
    "\n",
    "# Only keep the rows where the 'Year' column is '2016'\n",
    "df_rtdu['Year'] = df_rtdu['Year'].str.strip()\n",
    "df_rtdu = df_rtdu[df_rtdu['Year'].isin(['2016'])]\n",
    "\n",
    "# Remove the 'h' from the 'Passengers' column\n",
    "df_rtdu['Passengers'] = df_rtdu['Passengers'].str.replace('h', '', regex=False)\n",
    "\n",
    "# Delete missing values\n",
    "df_rtdu.dropna(inplace=True)\n",
    "\n",
    "# Add the column 'Estimated_number_of_road_traffic_deaths' from the 'df_rtdsfu' DataFrame to the 'df_rtdu' DataFrame\n",
    "df_rtdu_cleaned = pd.merge(df_rtdu, df_rtdsfu, on='Country', how='inner')\n",
    "\n",
    "# Convert the 'Passengers', 'Pedestrians' and 'Total' columns to numeric\n",
    "df_rtdu_cleaned['Passengers'] = pd.to_numeric(df_rtdu_cleaned['Passengers'], errors='coerce')\n",
    "df_rtdu_cleaned['Pedestrians'] = pd.to_numeric(df_rtdu_cleaned['Pedestrians'], errors='coerce')\n",
    "df_rtdu_cleaned['Total'] = pd.to_numeric(df_rtdu_cleaned['Total'], errors='coerce')\n",
    "\n",
    "# Add a column with the estimated number of road traffic deaths for passengers and pedestrians\n",
    "df_rtdu_cleaned['Passengers'] = df_rtdu_cleaned['Passengers']/100 * df_rtdu_cleaned['Total']\n",
    "df_rtdu_cleaned['Pedestrians'] = df_rtdu_cleaned['Pedestrians']/100 * df_rtdu_cleaned['Total']\n",
    "\n",
    "# Delete the 'Year' and 'Total' column\n",
    "df_rtdu_cleaned.drop(columns=['Year','Total'], inplace=True)\n",
    "\n",
    "# Convert columns to numeric type\n",
    "columns_to_normalize = ['Passengers','Pedestrians']\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    df_rtdu_cleaned[col] = pd.to_numeric(df_rtdu_cleaned[col])\n",
    "\n",
    "# Min-Max Normalization directly in the existing DataFrame\n",
    "#for col in columns_to_normalize:\n",
    "#    df_rtdu_cleaned[col] = (df_rtdu_cleaned[col] - df_rtdu_cleaned[col].min()) / (df_rtdu_cleaned[col].max() - df_rtdu_cleaned[col].min())\n",
    "\n",
    "# Change the 'Country' column to ISO codes\n",
    "# Extract the 'Country' column as a list\n",
    "country_names = df_rtdu_cleaned['Country'].tolist()\n",
    "\n",
    "# Create a mapping of country names to ISO codes\n",
    "country_to_iso = {}\n",
    "for country in country_names:\n",
    "    try:\n",
    "        # Get the country object using the name\n",
    "        country_obj = pycountry.countries.lookup(country)\n",
    "        # Map country name to ISO code\n",
    "        country_to_iso[country] = country_obj.alpha_3  # Using 3-letter ISO code\n",
    "    except LookupError:\n",
    "        # If the country is not found, handle it as needed\n",
    "        print(f\"Country not found: {country}\")\n",
    "\n",
    "# Manual mapping for countries that did not get an ISO code\n",
    "manual_iso_mapping = {\n",
    "    \"Bolivia (Plurinational State of)\": \"BOL\",\n",
    "    \"Cote d'Ivoire\": \"CIV\",  # Côte d'Ivoire\n",
    "    \"Democratic Republic of the Congo\": \"COD\",\n",
    "    \"Iran (Islamic Republic of)\": \"IRN\",\n",
    "    \"Micronesia (Federated States of)\": \"FSM\",\n",
    "    \"Netherlands (Kingdom of the)\": \"NLD\",\n",
    "    \"Republic of Korea\": \"KOR\"  # Use KOR for South Korea\n",
    "}\n",
    "\n",
    "# Update the mapping with the manual mappings\n",
    "country_to_iso.update(manual_iso_mapping)\n",
    "\n",
    "# Create a Series from the mapping to use for replacing values\n",
    "iso_codes_series = pd.Series(country_to_iso)\n",
    "\n",
    "# Replace the 'Country' column values with ISO codes\n",
    "df_rtdu_cleaned['Country'] = df_rtdu_cleaned['Country'].replace(iso_codes_series)\n",
    "\n",
    "df_rtdu_cleaned.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Pedestrians</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.613485</td>\n",
       "      <td>4.193465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.716367</td>\n",
       "      <td>3.757972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.277600</td>\n",
       "      <td>1.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.167600</td>\n",
       "      <td>3.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.126200</td>\n",
       "      <td>5.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.553400</td>\n",
       "      <td>17.792500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Passengers  Pedestrians\n",
       "count  117.000000   117.000000\n",
       "mean     5.613485     4.193465\n",
       "std      4.716367     3.757972\n",
       "min      0.000000     0.000000\n",
       "25%      2.277600     1.435000\n",
       "50%      4.167600     3.633300\n",
       "75%      7.126200     5.567100\n",
       "max     19.553400    17.792500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rtdu_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Moral Machine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'ExtendedSessionID' column from the Moral Machine dataset into a DataFrame\n",
    "extendedsessionid = pd.read_csv('Data/SharedResponses.csv', usecols=['ExtendedSessionID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of session ids to keep: 1105760\n"
     ]
    }
   ],
   "source": [
    "# Filter out the sessions that have between 24 and 26 responses\n",
    "session_counts = extendedsessionid['ExtendedSessionID'].value_counts()\n",
    "ids_to_keep = session_counts[(session_counts >= 24) & (session_counts <= 26)].index\n",
    "\n",
    "print(f\"Number of session ids to keep: {len(ids_to_keep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Finished processing chunk 1\n",
      "Processing chunk 2\n",
      "Finished processing chunk 2\n",
      "Processing chunk 3\n",
      "Finished processing chunk 3\n",
      "Processing chunk 4\n",
      "Finished processing chunk 4\n",
      "Processing chunk 5\n",
      "Finished processing chunk 5\n",
      "Processing chunk 6\n",
      "Finished processing chunk 6\n",
      "Processing chunk 7\n",
      "Finished processing chunk 7\n",
      "Processing chunk 8\n",
      "Finished processing chunk 8\n",
      "Processing chunk 9\n",
      "Finished processing chunk 9\n",
      "Processing chunk 10\n",
      "Finished processing chunk 10\n",
      "Processing chunk 11\n",
      "Finished processing chunk 11\n",
      "Processing chunk 12\n",
      "Finished processing chunk 12\n",
      "Processing chunk 13\n",
      "Finished processing chunk 13\n",
      "Processing chunk 14\n",
      "Finished processing chunk 14\n",
      "Processing chunk 15\n",
      "Finished processing chunk 15\n",
      "Processing chunk 16\n",
      "Finished processing chunk 16\n",
      "Processing chunk 17\n",
      "Finished processing chunk 17\n",
      "Processing chunk 18\n",
      "Finished processing chunk 18\n",
      "Processing chunk 19\n",
      "Finished processing chunk 19\n",
      "Processing chunk 20\n",
      "Finished processing chunk 20\n",
      "Processing chunk 21\n",
      "Finished processing chunk 21\n",
      "Processing chunk 22\n",
      "Finished processing chunk 22\n",
      "Processing chunk 23\n",
      "Finished processing chunk 23\n",
      "Processing chunk 24\n",
      "Finished processing chunk 24\n",
      "Processing chunk 25\n",
      "Finished processing chunk 25\n",
      "Processing chunk 26\n",
      "Finished processing chunk 26\n",
      "Processing chunk 27\n",
      "Finished processing chunk 27\n",
      "Processing chunk 28\n",
      "Finished processing chunk 28\n",
      "Processing chunk 29\n",
      "Finished processing chunk 29\n",
      "Processing chunk 30\n",
      "Finished processing chunk 30\n",
      "Processing chunk 31\n",
      "Finished processing chunk 31\n",
      "Processing chunk 32\n",
      "Finished processing chunk 32\n",
      "Processing chunk 33\n",
      "Finished processing chunk 33\n",
      "Processing chunk 34\n",
      "Finished processing chunk 34\n",
      "Processing chunk 35\n",
      "Finished processing chunk 35\n",
      "Processing chunk 36\n",
      "Finished processing chunk 36\n",
      "Processing chunk 37\n",
      "Finished processing chunk 37\n",
      "Processing chunk 38\n",
      "Finished processing chunk 38\n",
      "Processing chunk 39\n",
      "Finished processing chunk 39\n",
      "Processing chunk 40\n",
      "Finished processing chunk 40\n",
      "Processing chunk 41\n",
      "Finished processing chunk 41\n",
      "Processing chunk 42\n",
      "Finished processing chunk 42\n",
      "Processing chunk 43\n",
      "Finished processing chunk 43\n",
      "Processing chunk 44\n",
      "Finished processing chunk 44\n",
      "Processing chunk 45\n",
      "Finished processing chunk 45\n",
      "Processing chunk 46\n",
      "Finished processing chunk 46\n",
      "Processing chunk 47\n",
      "Finished processing chunk 47\n",
      "Processing chunk 48\n",
      "Finished processing chunk 48\n",
      "Processing chunk 49\n",
      "Finished processing chunk 49\n",
      "Processing chunk 50\n",
      "Finished processing chunk 50\n",
      "Processing chunk 51\n",
      "Finished processing chunk 51\n",
      "Processing chunk 52\n",
      "Finished processing chunk 52\n",
      "Processing chunk 53\n",
      "Finished processing chunk 53\n",
      "Processing chunk 54\n",
      "Finished processing chunk 54\n",
      "Processing chunk 55\n",
      "Finished processing chunk 55\n",
      "Processing chunk 56\n",
      "Finished processing chunk 56\n",
      "Processing chunk 57\n",
      "Finished processing chunk 57\n",
      "Processing chunk 58\n",
      "Finished processing chunk 58\n",
      "Processing chunk 59\n",
      "Finished processing chunk 59\n",
      "Processing chunk 60\n",
      "Finished processing chunk 60\n",
      "Processing chunk 61\n",
      "Finished processing chunk 61\n",
      "Processing chunk 62\n",
      "Finished processing chunk 62\n",
      "Processing chunk 63\n",
      "Finished processing chunk 63\n",
      "Processing chunk 64\n",
      "Finished processing chunk 64\n",
      "Processing chunk 65\n",
      "Finished processing chunk 65\n",
      "Processing chunk 66\n",
      "Finished processing chunk 66\n",
      "Processing chunk 67\n",
      "Finished processing chunk 67\n",
      "Processing chunk 68\n",
      "Finished processing chunk 68\n",
      "Processing chunk 69\n",
      "Finished processing chunk 69\n",
      "Processing chunk 70\n",
      "Finished processing chunk 70\n",
      "Processing chunk 71\n",
      "Finished processing chunk 71\n",
      "Processing chunk 72\n",
      "Finished processing chunk 72\n",
      "Processing chunk 73\n",
      "Finished processing chunk 73\n",
      "Processing chunk 74\n",
      "Finished processing chunk 74\n",
      "Processing chunk 75\n",
      "Finished processing chunk 75\n",
      "Processing chunk 76\n",
      "Finished processing chunk 76\n",
      "Processing chunk 77\n",
      "Finished processing chunk 77\n",
      "Processing chunk 78\n",
      "Finished processing chunk 78\n",
      "Processing chunk 79\n",
      "Finished processing chunk 79\n",
      "Processing chunk 80\n",
      "Finished processing chunk 80\n",
      "Processing chunk 81\n",
      "Finished processing chunk 81\n",
      "Processing chunk 82\n",
      "Finished processing chunk 82\n",
      "Processing chunk 83\n",
      "Finished processing chunk 83\n",
      "Processing chunk 84\n",
      "Finished processing chunk 84\n",
      "Processing chunk 85\n",
      "Finished processing chunk 85\n",
      "Processing chunk 86\n",
      "Finished processing chunk 86\n",
      "Processing chunk 87\n",
      "Finished processing chunk 87\n",
      "Processing chunk 88\n",
      "Finished processing chunk 88\n",
      "Processing chunk 89\n",
      "Finished processing chunk 89\n",
      "Processing chunk 90\n",
      "Finished processing chunk 90\n",
      "Processing chunk 91\n",
      "Finished processing chunk 91\n",
      "Processing chunk 92\n",
      "Finished processing chunk 92\n",
      "Processing chunk 93\n",
      "Finished processing chunk 93\n",
      "Processing chunk 94\n",
      "Finished processing chunk 94\n",
      "Processing chunk 95\n",
      "Finished processing chunk 95\n",
      "Processing chunk 96\n",
      "Finished processing chunk 96\n",
      "Processing chunk 97\n",
      "Finished processing chunk 97\n",
      "Processing chunk 98\n",
      "Finished processing chunk 98\n",
      "Processing chunk 99\n",
      "Finished processing chunk 99\n",
      "Processing chunk 100\n",
      "Finished processing chunk 100\n",
      "Processing chunk 101\n",
      "Finished processing chunk 101\n",
      "Processing chunk 102\n",
      "Finished processing chunk 102\n",
      "Processing chunk 103\n",
      "Finished processing chunk 103\n",
      "Processing chunk 104\n",
      "Finished processing chunk 104\n",
      "Processing chunk 105\n",
      "Finished processing chunk 105\n",
      "Processing chunk 106\n",
      "Finished processing chunk 106\n",
      "Processing chunk 107\n",
      "Finished processing chunk 107\n",
      "Processing chunk 108\n",
      "Finished processing chunk 108\n",
      "Processing chunk 109\n",
      "Finished processing chunk 109\n",
      "Processing chunk 110\n",
      "Finished processing chunk 110\n",
      "Processing chunk 111\n",
      "Finished processing chunk 111\n",
      "Processing chunk 112\n",
      "Finished processing chunk 112\n",
      "Processing chunk 113\n",
      "Finished processing chunk 113\n",
      "Processing chunk 114\n",
      "Finished processing chunk 114\n",
      "Processing chunk 115\n",
      "Finished processing chunk 115\n",
      "Processing chunk 116\n",
      "Finished processing chunk 116\n",
      "Processing chunk 117\n",
      "Finished processing chunk 117\n",
      "Processing chunk 118\n",
      "Finished processing chunk 118\n",
      "Processing chunk 119\n",
      "Finished processing chunk 119\n",
      "Processing chunk 120\n",
      "Finished processing chunk 120\n",
      "Processing chunk 121\n",
      "Finished processing chunk 121\n",
      "Processing chunk 122\n",
      "Finished processing chunk 122\n",
      "Processing chunk 123\n",
      "Finished processing chunk 123\n",
      "Processing chunk 124\n",
      "Finished processing chunk 124\n",
      "Processing chunk 125\n",
      "Finished processing chunk 125\n",
      "Processing chunk 126\n",
      "Finished processing chunk 126\n",
      "Processing chunk 127\n",
      "Finished processing chunk 127\n",
      "Processing chunk 128\n",
      "Finished processing chunk 128\n",
      "Processing chunk 129\n",
      "Finished processing chunk 129\n",
      "Processing chunk 130\n",
      "Finished processing chunk 130\n",
      "Processing chunk 131\n",
      "Finished processing chunk 131\n",
      "Processing chunk 132\n",
      "Finished processing chunk 132\n",
      "Processing chunk 133\n",
      "Finished processing chunk 133\n",
      "Processing chunk 134\n",
      "Finished processing chunk 134\n",
      "Processing chunk 135\n",
      "Finished processing chunk 135\n",
      "Processing chunk 136\n",
      "Finished processing chunk 136\n",
      "Processing chunk 137\n",
      "Finished processing chunk 137\n",
      "Processing chunk 138\n",
      "Finished processing chunk 138\n",
      "Processing chunk 139\n",
      "Finished processing chunk 139\n",
      "Processing chunk 140\n",
      "Finished processing chunk 140\n",
      "Processing chunk 141\n",
      "Finished processing chunk 141\n",
      "Processing chunk 142\n",
      "Finished processing chunk 142\n",
      "Processing chunk 143\n",
      "Finished processing chunk 143\n",
      "Processing chunk 144\n",
      "Finished processing chunk 144\n",
      "Processing chunk 145\n",
      "Finished processing chunk 145\n",
      "Processing chunk 146\n",
      "Finished processing chunk 146\n",
      "Processing chunk 147\n",
      "Finished processing chunk 147\n",
      "Processing chunk 148\n",
      "Finished processing chunk 148\n",
      "Processing chunk 149\n",
      "Finished processing chunk 149\n",
      "Processing chunk 150\n",
      "Finished processing chunk 150\n",
      "Processing chunk 151\n",
      "Finished processing chunk 151\n",
      "Processing chunk 152\n",
      "Finished processing chunk 152\n",
      "Processing chunk 153\n",
      "Finished processing chunk 153\n",
      "Processing chunk 154\n",
      "Finished processing chunk 154\n",
      "Processing chunk 155\n",
      "Finished processing chunk 155\n",
      "Processing chunk 156\n",
      "Finished processing chunk 156\n",
      "Processing chunk 157\n",
      "Finished processing chunk 157\n",
      "Processing chunk 158\n",
      "Finished processing chunk 158\n",
      "Processing chunk 159\n",
      "Finished processing chunk 159\n",
      "Processing chunk 160\n",
      "Finished processing chunk 160\n",
      "Processing chunk 161\n",
      "Finished processing chunk 161\n",
      "Processing chunk 162\n",
      "Finished processing chunk 162\n",
      "Processing chunk 163\n",
      "Finished processing chunk 163\n",
      "Processing chunk 164\n",
      "Finished processing chunk 164\n",
      "Processing chunk 165\n",
      "Finished processing chunk 165\n",
      "Processing chunk 166\n",
      "Finished processing chunk 166\n",
      "Processing chunk 167\n",
      "Finished processing chunk 167\n",
      "Processing chunk 168\n",
      "Finished processing chunk 168\n",
      "Processing chunk 169\n",
      "Finished processing chunk 169\n",
      "Processing chunk 170\n",
      "Finished processing chunk 170\n",
      "Processing chunk 171\n",
      "Finished processing chunk 171\n",
      "Processing chunk 172\n",
      "Finished processing chunk 172\n",
      "Processing chunk 173\n",
      "Finished processing chunk 173\n",
      "Processing chunk 174\n",
      "Finished processing chunk 174\n",
      "Processing chunk 175\n",
      "Finished processing chunk 175\n",
      "Processing chunk 176\n",
      "Finished processing chunk 176\n",
      "Processing chunk 177\n",
      "Finished processing chunk 177\n",
      "Processing chunk 178\n",
      "Finished processing chunk 178\n",
      "Processing chunk 179\n",
      "Finished processing chunk 179\n",
      "Processing chunk 180\n",
      "Finished processing chunk 180\n",
      "Processing chunk 181\n",
      "Finished processing chunk 181\n",
      "Processing chunk 182\n",
      "Finished processing chunk 182\n",
      "Processing chunk 183\n",
      "Finished processing chunk 183\n",
      "Processing chunk 184\n",
      "Finished processing chunk 184\n",
      "Processing chunk 185\n",
      "Finished processing chunk 185\n",
      "Processing chunk 186\n",
      "Finished processing chunk 186\n",
      "Processing chunk 187\n",
      "Finished processing chunk 187\n",
      "Processing chunk 188\n",
      "Finished processing chunk 188\n",
      "Processing chunk 189\n",
      "Finished processing chunk 189\n",
      "Processing chunk 190\n",
      "Finished processing chunk 190\n",
      "Processing chunk 191\n",
      "Finished processing chunk 191\n",
      "Processing chunk 192\n",
      "Finished processing chunk 192\n",
      "Processing chunk 193\n",
      "Finished processing chunk 193\n",
      "Processing chunk 194\n",
      "Finished processing chunk 194\n",
      "Processing chunk 195\n",
      "Finished processing chunk 195\n",
      "Processing chunk 196\n",
      "Finished processing chunk 196\n",
      "Processing chunk 197\n",
      "Finished processing chunk 197\n",
      "Processing chunk 198\n",
      "Finished processing chunk 198\n",
      "Processing chunk 199\n",
      "Finished processing chunk 199\n",
      "Processing chunk 200\n",
      "Finished processing chunk 200\n",
      "Processing chunk 201\n",
      "Finished processing chunk 201\n",
      "Processing chunk 202\n",
      "Finished processing chunk 202\n",
      "Processing chunk 203\n",
      "Finished processing chunk 203\n",
      "Processing chunk 204\n",
      "Finished processing chunk 204\n",
      "Processing chunk 205\n",
      "Finished processing chunk 205\n",
      "Processing chunk 206\n",
      "Finished processing chunk 206\n",
      "Processing chunk 207\n",
      "Finished processing chunk 207\n",
      "Processing chunk 208\n",
      "Finished processing chunk 208\n",
      "Processing chunk 209\n",
      "Finished processing chunk 209\n",
      "Processing chunk 210\n",
      "Finished processing chunk 210\n",
      "Processing chunk 211\n",
      "Finished processing chunk 211\n",
      "Processing chunk 212\n",
      "Finished processing chunk 212\n",
      "Processing chunk 213\n",
      "Finished processing chunk 213\n",
      "Processing chunk 214\n",
      "Finished processing chunk 214\n",
      "Processing chunk 215\n",
      "Finished processing chunk 215\n",
      "Processing chunk 216\n",
      "Finished processing chunk 216\n",
      "Processing chunk 217\n",
      "Finished processing chunk 217\n",
      "Processing chunk 218\n",
      "Finished processing chunk 218\n",
      "Processing chunk 219\n",
      "Finished processing chunk 219\n",
      "Processing chunk 220\n",
      "Finished processing chunk 220\n",
      "Processing chunk 221\n",
      "Finished processing chunk 221\n",
      "Processing chunk 222\n",
      "Finished processing chunk 222\n",
      "Processing chunk 223\n",
      "Finished processing chunk 223\n",
      "Processing chunk 224\n",
      "Finished processing chunk 224\n",
      "Processing chunk 225\n",
      "Finished processing chunk 225\n",
      "Processing chunk 226\n",
      "Finished processing chunk 226\n",
      "Processing chunk 227\n",
      "Finished processing chunk 227\n",
      "Processing chunk 228\n",
      "Finished processing chunk 228\n",
      "Processing chunk 229\n",
      "Finished processing chunk 229\n",
      "Processing chunk 230\n",
      "Finished processing chunk 230\n",
      "Processing chunk 231\n",
      "Finished processing chunk 231\n",
      "Processing chunk 232\n",
      "Finished processing chunk 232\n",
      "Processing chunk 233\n",
      "Finished processing chunk 233\n",
      "Processing chunk 234\n",
      "Finished processing chunk 234\n",
      "Processing chunk 235\n",
      "Finished processing chunk 235\n",
      "Processing chunk 236\n",
      "Finished processing chunk 236\n",
      "Processing chunk 237\n",
      "Finished processing chunk 237\n",
      "Processing chunk 238\n",
      "Finished processing chunk 238\n",
      "Processing chunk 239\n",
      "Finished processing chunk 239\n",
      "Processing chunk 240\n",
      "Finished processing chunk 240\n",
      "Processing chunk 241\n",
      "Finished processing chunk 241\n",
      "Processing chunk 242\n",
      "Finished processing chunk 242\n",
      "Processing chunk 243\n",
      "Finished processing chunk 243\n",
      "Processing chunk 244\n",
      "Finished processing chunk 244\n",
      "Processing chunk 245\n",
      "Finished processing chunk 245\n",
      "Processing chunk 246\n",
      "Finished processing chunk 246\n",
      "Processing chunk 247\n",
      "Finished processing chunk 247\n",
      "Processing chunk 248\n",
      "Finished processing chunk 248\n",
      "Processing chunk 249\n",
      "Finished processing chunk 249\n",
      "Processing chunk 250\n",
      "Finished processing chunk 250\n",
      "Processing chunk 251\n",
      "Finished processing chunk 251\n",
      "Processing chunk 252\n",
      "Finished processing chunk 252\n",
      "Processing chunk 253\n",
      "Finished processing chunk 253\n",
      "Processing chunk 254\n",
      "Finished processing chunk 254\n",
      "Processing chunk 255\n",
      "Finished processing chunk 255\n",
      "Processing chunk 256\n",
      "Finished processing chunk 256\n",
      "Processing chunk 257\n",
      "Finished processing chunk 257\n",
      "Processing chunk 258\n",
      "Finished processing chunk 258\n",
      "Processing chunk 259\n",
      "Finished processing chunk 259\n",
      "Processing chunk 260\n",
      "Finished processing chunk 260\n",
      "Processing chunk 261\n",
      "Finished processing chunk 261\n",
      "Processing chunk 262\n",
      "Finished processing chunk 262\n",
      "Processing chunk 263\n",
      "Finished processing chunk 263\n",
      "Processing chunk 264\n",
      "Finished processing chunk 264\n",
      "Processing chunk 265\n",
      "Finished processing chunk 265\n",
      "Processing chunk 266\n",
      "Finished processing chunk 266\n",
      "Processing chunk 267\n",
      "Finished processing chunk 267\n",
      "Processing chunk 268\n",
      "Finished processing chunk 268\n",
      "Processing chunk 269\n",
      "Finished processing chunk 269\n",
      "Processing chunk 270\n",
      "Finished processing chunk 270\n",
      "Processing chunk 271\n",
      "Finished processing chunk 271\n",
      "Processing chunk 272\n",
      "Finished processing chunk 272\n",
      "Processing chunk 273\n",
      "Finished processing chunk 273\n",
      "Processing chunk 274\n",
      "Finished processing chunk 274\n",
      "Processing chunk 275\n",
      "Finished processing chunk 275\n",
      "Processing chunk 276\n",
      "Finished processing chunk 276\n",
      "Processing chunk 277\n",
      "Finished processing chunk 277\n",
      "Processing chunk 278\n",
      "Finished processing chunk 278\n",
      "Processing chunk 279\n",
      "Finished processing chunk 279\n",
      "Processing chunk 280\n",
      "Finished processing chunk 280\n",
      "Processing chunk 281\n",
      "Finished processing chunk 281\n",
      "Processing chunk 282\n",
      "Finished processing chunk 282\n",
      "Processing chunk 283\n",
      "Finished processing chunk 283\n",
      "Processing chunk 284\n",
      "Finished processing chunk 284\n",
      "Processing chunk 285\n",
      "Finished processing chunk 285\n",
      "Processing chunk 286\n",
      "Finished processing chunk 286\n",
      "Processing chunk 287\n",
      "Finished processing chunk 287\n",
      "Processing chunk 288\n",
      "Finished processing chunk 288\n",
      "Processing chunk 289\n",
      "Finished processing chunk 289\n",
      "Processing chunk 290\n",
      "Finished processing chunk 290\n",
      "Processing chunk 291\n",
      "Finished processing chunk 291\n",
      "Processing chunk 292\n",
      "Finished processing chunk 292\n",
      "Processing chunk 293\n",
      "Finished processing chunk 293\n",
      "Processing chunk 294\n",
      "Finished processing chunk 294\n",
      "Processing chunk 295\n",
      "Finished processing chunk 295\n",
      "Processing chunk 296\n",
      "Finished processing chunk 296\n",
      "Processing chunk 297\n",
      "Finished processing chunk 297\n",
      "Processing chunk 298\n",
      "Finished processing chunk 298\n",
      "Processing chunk 299\n",
      "Finished processing chunk 299\n",
      "Processing chunk 300\n",
      "Finished processing chunk 300\n",
      "Processing chunk 301\n",
      "Finished processing chunk 301\n",
      "Processing chunk 302\n",
      "Finished processing chunk 302\n",
      "Processing chunk 303\n",
      "Finished processing chunk 303\n",
      "Processing chunk 304\n",
      "Finished processing chunk 304\n",
      "Processing chunk 305\n",
      "Finished processing chunk 305\n",
      "Processing chunk 306\n",
      "Finished processing chunk 306\n",
      "Processing chunk 307\n",
      "Finished processing chunk 307\n",
      "Processing chunk 308\n",
      "Finished processing chunk 308\n",
      "Processing chunk 309\n",
      "Finished processing chunk 309\n",
      "Processing chunk 310\n",
      "Finished processing chunk 310\n",
      "Processing chunk 311\n",
      "Finished processing chunk 311\n",
      "Processing chunk 312\n",
      "Finished processing chunk 312\n",
      "Processing chunk 313\n",
      "Finished processing chunk 313\n",
      "Processing chunk 314\n",
      "Finished processing chunk 314\n",
      "Processing chunk 315\n",
      "Finished processing chunk 315\n",
      "Processing chunk 316\n",
      "Finished processing chunk 316\n",
      "Processing chunk 317\n",
      "Finished processing chunk 317\n",
      "Processing chunk 318\n",
      "Finished processing chunk 318\n",
      "Processing chunk 319\n",
      "Finished processing chunk 319\n",
      "Processing chunk 320\n",
      "Finished processing chunk 320\n",
      "Processing chunk 321\n",
      "Finished processing chunk 321\n",
      "Processing chunk 322\n",
      "Finished processing chunk 322\n",
      "Processing chunk 323\n",
      "Finished processing chunk 323\n",
      "Processing chunk 324\n",
      "Finished processing chunk 324\n",
      "Processing chunk 325\n",
      "Finished processing chunk 325\n",
      "Processing chunk 326\n",
      "Finished processing chunk 326\n",
      "Processing chunk 327\n",
      "Finished processing chunk 327\n",
      "Processing chunk 328\n",
      "Finished processing chunk 328\n",
      "Processing chunk 329\n",
      "Finished processing chunk 329\n",
      "Processing chunk 330\n",
      "Finished processing chunk 330\n",
      "Processing chunk 331\n",
      "Finished processing chunk 331\n",
      "Processing chunk 332\n",
      "Finished processing chunk 332\n",
      "Processing chunk 333\n",
      "Finished processing chunk 333\n",
      "Processing chunk 334\n",
      "Finished processing chunk 334\n",
      "Processing chunk 335\n",
      "Finished processing chunk 335\n",
      "Processing chunk 336\n",
      "Finished processing chunk 336\n",
      "Processing chunk 337\n",
      "Finished processing chunk 337\n",
      "Processing chunk 338\n",
      "Finished processing chunk 338\n",
      "Processing chunk 339\n",
      "Finished processing chunk 339\n",
      "Processing chunk 340\n",
      "Finished processing chunk 340\n",
      "Processing chunk 341\n",
      "Finished processing chunk 341\n",
      "Processing chunk 342\n",
      "Finished processing chunk 342\n",
      "Processing chunk 343\n",
      "Finished processing chunk 343\n",
      "Processing chunk 344\n",
      "Finished processing chunk 344\n",
      "Processing chunk 345\n",
      "Finished processing chunk 345\n",
      "Processing chunk 346\n",
      "Finished processing chunk 346\n",
      "Processing chunk 347\n",
      "Finished processing chunk 347\n",
      "Processing chunk 348\n",
      "Finished processing chunk 348\n",
      "Processing chunk 349\n",
      "Finished processing chunk 349\n",
      "Processing chunk 350\n",
      "Finished processing chunk 350\n",
      "Processing chunk 351\n",
      "Finished processing chunk 351\n",
      "Processing chunk 352\n",
      "Finished processing chunk 352\n",
      "Processing chunk 353\n",
      "Finished processing chunk 353\n",
      "Processing chunk 354\n",
      "Finished processing chunk 354\n",
      "Processing chunk 355\n",
      "Finished processing chunk 355\n",
      "Processing chunk 356\n",
      "Finished processing chunk 356\n",
      "Processing chunk 357\n",
      "Finished processing chunk 357\n",
      "Processing chunk 358\n",
      "Finished processing chunk 358\n",
      "Processing chunk 359\n",
      "Finished processing chunk 359\n",
      "Processing chunk 360\n",
      "Finished processing chunk 360\n",
      "Processing chunk 361\n",
      "Finished processing chunk 361\n",
      "Processing chunk 362\n",
      "Finished processing chunk 362\n",
      "Processing chunk 363\n",
      "Finished processing chunk 363\n",
      "Processing chunk 364\n",
      "Finished processing chunk 364\n",
      "Processing chunk 365\n",
      "Finished processing chunk 365\n",
      "Processing chunk 366\n",
      "Finished processing chunk 366\n",
      "Processing chunk 367\n",
      "Finished processing chunk 367\n",
      "Processing chunk 368\n",
      "Finished processing chunk 368\n",
      "Processing chunk 369\n",
      "Finished processing chunk 369\n",
      "Processing chunk 370\n",
      "Finished processing chunk 370\n",
      "Processing chunk 371\n",
      "Finished processing chunk 371\n",
      "Processing chunk 372\n",
      "Finished processing chunk 372\n",
      "Processing chunk 373\n",
      "Finished processing chunk 373\n",
      "Processing chunk 374\n",
      "Finished processing chunk 374\n",
      "Processing chunk 375\n",
      "Finished processing chunk 375\n",
      "Processing chunk 376\n",
      "Finished processing chunk 376\n",
      "Processing chunk 377\n",
      "Finished processing chunk 377\n",
      "Processing chunk 378\n",
      "Finished processing chunk 378\n",
      "Processing chunk 379\n",
      "Finished processing chunk 379\n",
      "Processing chunk 380\n",
      "Finished processing chunk 380\n",
      "Processing chunk 381\n",
      "Finished processing chunk 381\n",
      "Processing chunk 382\n",
      "Finished processing chunk 382\n",
      "Processing chunk 383\n",
      "Finished processing chunk 383\n",
      "Processing chunk 384\n",
      "Finished processing chunk 384\n",
      "Processing chunk 385\n",
      "Finished processing chunk 385\n",
      "Processing chunk 386\n",
      "Finished processing chunk 386\n",
      "Processing chunk 387\n",
      "Finished processing chunk 387\n",
      "Processing chunk 388\n",
      "Finished processing chunk 388\n",
      "Processing chunk 389\n",
      "Finished processing chunk 389\n",
      "Processing chunk 390\n",
      "Finished processing chunk 390\n",
      "Processing chunk 391\n",
      "Finished processing chunk 391\n",
      "Processing chunk 392\n",
      "Finished processing chunk 392\n",
      "Processing chunk 393\n",
      "Finished processing chunk 393\n",
      "Processing chunk 394\n",
      "Finished processing chunk 394\n",
      "Processing chunk 395\n",
      "Finished processing chunk 395\n",
      "Processing chunk 396\n",
      "Finished processing chunk 396\n",
      "Processing chunk 397\n",
      "Finished processing chunk 397\n",
      "Processing chunk 398\n",
      "Finished processing chunk 398\n",
      "Processing chunk 399\n",
      "Finished processing chunk 399\n",
      "Processing chunk 400\n",
      "Finished processing chunk 400\n",
      "Processing chunk 401\n",
      "Finished processing chunk 401\n",
      "Processing chunk 402\n",
      "Finished processing chunk 402\n",
      "Processing chunk 403\n",
      "Finished processing chunk 403\n",
      "Processing chunk 404\n",
      "Finished processing chunk 404\n",
      "Processing chunk 405\n",
      "Finished processing chunk 405\n",
      "Processing chunk 406\n",
      "Finished processing chunk 406\n",
      "Processing chunk 407\n",
      "Finished processing chunk 407\n",
      "Processing chunk 408\n",
      "Finished processing chunk 408\n",
      "Processing chunk 409\n",
      "Finished processing chunk 409\n",
      "Processing chunk 410\n",
      "Finished processing chunk 410\n",
      "Processing chunk 411\n",
      "Finished processing chunk 411\n",
      "Processing chunk 412\n",
      "Finished processing chunk 412\n",
      "Processing chunk 413\n",
      "Finished processing chunk 413\n",
      "Processing chunk 414\n",
      "Finished processing chunk 414\n",
      "Processing chunk 415\n",
      "Finished processing chunk 415\n",
      "Processing chunk 416\n",
      "Finished processing chunk 416\n",
      "Processing chunk 417\n",
      "Finished processing chunk 417\n",
      "Processing chunk 418\n",
      "Finished processing chunk 418\n",
      "Processing chunk 419\n",
      "Finished processing chunk 419\n",
      "Processing chunk 420\n",
      "Finished processing chunk 420\n",
      "Processing chunk 421\n",
      "Finished processing chunk 421\n",
      "Processing chunk 422\n",
      "Finished processing chunk 422\n",
      "Processing chunk 423\n",
      "Finished processing chunk 423\n",
      "Processing chunk 424\n",
      "Finished processing chunk 424\n",
      "Processing chunk 425\n",
      "Finished processing chunk 425\n",
      "Processing chunk 426\n",
      "Finished processing chunk 426\n",
      "Processing chunk 427\n",
      "Finished processing chunk 427\n",
      "Processing chunk 428\n",
      "Finished processing chunk 428\n",
      "Processing chunk 429\n",
      "Finished processing chunk 429\n",
      "Processing chunk 430\n",
      "Finished processing chunk 430\n",
      "Processing chunk 431\n",
      "Finished processing chunk 431\n",
      "Processing chunk 432\n",
      "Finished processing chunk 432\n",
      "Processing chunk 433\n",
      "Finished processing chunk 433\n",
      "Processing chunk 434\n",
      "Finished processing chunk 434\n",
      "Processing chunk 435\n",
      "Finished processing chunk 435\n",
      "Processing chunk 436\n",
      "Finished processing chunk 436\n",
      "Processing chunk 437\n",
      "Finished processing chunk 437\n",
      "Processing chunk 438\n",
      "Finished processing chunk 438\n",
      "Processing chunk 439\n",
      "Finished processing chunk 439\n",
      "Processing chunk 440\n",
      "Finished processing chunk 440\n",
      "Processing chunk 441\n",
      "Finished processing chunk 441\n",
      "Processing chunk 442\n",
      "Finished processing chunk 442\n",
      "Processing chunk 443\n",
      "Finished processing chunk 443\n",
      "Processing chunk 444\n",
      "Finished processing chunk 444\n",
      "Processing chunk 445\n",
      "Finished processing chunk 445\n",
      "Processing chunk 446\n",
      "Finished processing chunk 446\n",
      "Processing chunk 447\n",
      "Finished processing chunk 447\n",
      "Processing chunk 448\n",
      "Finished processing chunk 448\n",
      "Processing chunk 449\n",
      "Finished processing chunk 449\n",
      "Processing chunk 450\n",
      "Finished processing chunk 450\n",
      "Processing chunk 451\n",
      "Finished processing chunk 451\n",
      "Processing chunk 452\n",
      "Finished processing chunk 452\n",
      "Processing chunk 453\n",
      "Finished processing chunk 453\n",
      "Processing chunk 454\n",
      "Finished processing chunk 454\n",
      "Processing chunk 455\n",
      "Finished processing chunk 455\n",
      "Processing chunk 456\n",
      "Finished processing chunk 456\n",
      "Processing chunk 457\n",
      "Finished processing chunk 457\n",
      "Processing chunk 458\n",
      "Finished processing chunk 458\n",
      "Processing chunk 459\n",
      "Finished processing chunk 459\n",
      "Processing chunk 460\n",
      "Finished processing chunk 460\n",
      "Processing chunk 461\n",
      "Finished processing chunk 461\n",
      "Processing chunk 462\n",
      "Finished processing chunk 462\n",
      "Processing chunk 463\n",
      "Finished processing chunk 463\n",
      "Processing chunk 464\n",
      "Finished processing chunk 464\n",
      "Processing chunk 465\n",
      "Finished processing chunk 465\n",
      "Processing chunk 466\n",
      "Finished processing chunk 466\n",
      "Processing chunk 467\n",
      "Finished processing chunk 467\n",
      "Processing chunk 468\n",
      "Finished processing chunk 468\n",
      "Processing chunk 469\n",
      "Finished processing chunk 469\n",
      "Processing chunk 470\n",
      "Finished processing chunk 470\n",
      "Processing chunk 471\n",
      "Finished processing chunk 471\n",
      "Processing chunk 472\n",
      "Finished processing chunk 472\n",
      "Processing chunk 473\n",
      "Finished processing chunk 473\n",
      "Processing chunk 474\n",
      "Finished processing chunk 474\n",
      "Processing chunk 475\n",
      "Finished processing chunk 475\n",
      "Processing chunk 476\n",
      "Finished processing chunk 476\n",
      "Processing chunk 477\n",
      "Finished processing chunk 477\n",
      "Processing chunk 478\n",
      "Finished processing chunk 478\n",
      "Processing chunk 479\n",
      "Finished processing chunk 479\n",
      "Processing chunk 480\n",
      "Finished processing chunk 480\n",
      "Processing chunk 481\n",
      "Finished processing chunk 481\n",
      "Processing chunk 482\n",
      "Finished processing chunk 482\n",
      "Processing chunk 483\n",
      "Finished processing chunk 483\n",
      "Processing chunk 484\n",
      "Finished processing chunk 484\n",
      "Processing chunk 485\n",
      "Finished processing chunk 485\n",
      "Processing chunk 486\n",
      "Finished processing chunk 486\n",
      "Processing chunk 487\n",
      "Finished processing chunk 487\n",
      "Processing chunk 488\n",
      "Finished processing chunk 488\n",
      "Processing chunk 489\n",
      "Finished processing chunk 489\n",
      "Processing chunk 490\n",
      "Finished processing chunk 490\n",
      "Processing chunk 491\n",
      "Finished processing chunk 491\n",
      "Processing chunk 492\n",
      "Finished processing chunk 492\n",
      "Processing chunk 493\n",
      "Finished processing chunk 493\n",
      "Processing chunk 494\n",
      "Finished processing chunk 494\n",
      "Processing chunk 495\n",
      "Finished processing chunk 495\n",
      "Processing chunk 496\n",
      "Finished processing chunk 496\n",
      "Processing chunk 497\n",
      "Finished processing chunk 497\n",
      "Processing chunk 498\n",
      "Finished processing chunk 498\n",
      "Processing chunk 499\n",
      "Finished processing chunk 499\n",
      "Processing chunk 500\n",
      "Finished processing chunk 500\n",
      "Processing chunk 501\n",
      "Finished processing chunk 501\n",
      "Processing chunk 502\n",
      "Finished processing chunk 502\n",
      "Processing chunk 503\n",
      "Finished processing chunk 503\n",
      "Processing chunk 504\n",
      "Finished processing chunk 504\n",
      "Processing chunk 505\n",
      "Finished processing chunk 505\n",
      "Processing chunk 506\n",
      "Finished processing chunk 506\n",
      "Processing chunk 507\n",
      "Finished processing chunk 507\n",
      "Processing chunk 508\n",
      "Finished processing chunk 508\n",
      "Processing chunk 509\n",
      "Finished processing chunk 509\n",
      "Processing chunk 510\n",
      "Finished processing chunk 510\n",
      "Processing chunk 511\n",
      "Finished processing chunk 511\n",
      "Processing chunk 512\n",
      "Finished processing chunk 512\n",
      "Processing chunk 513\n",
      "Finished processing chunk 513\n",
      "Processing chunk 514\n",
      "Finished processing chunk 514\n",
      "Processing chunk 515\n",
      "Finished processing chunk 515\n",
      "Processing chunk 516\n",
      "Finished processing chunk 516\n",
      "Processing chunk 517\n",
      "Finished processing chunk 517\n",
      "Processing chunk 518\n",
      "Finished processing chunk 518\n",
      "Processing chunk 519\n",
      "Finished processing chunk 519\n",
      "Processing chunk 520\n",
      "Finished processing chunk 520\n",
      "Processing chunk 521\n",
      "Finished processing chunk 521\n",
      "Processing chunk 522\n",
      "Finished processing chunk 522\n",
      "Processing chunk 523\n",
      "Finished processing chunk 523\n",
      "Processing chunk 524\n",
      "Finished processing chunk 524\n",
      "Processing chunk 525\n",
      "Finished processing chunk 525\n",
      "Processing chunk 526\n",
      "Finished processing chunk 526\n",
      "Processing chunk 527\n",
      "Finished processing chunk 527\n",
      "Processing chunk 528\n",
      "Finished processing chunk 528\n",
      "Processing chunk 529\n",
      "Finished processing chunk 529\n",
      "Processing chunk 530\n",
      "Finished processing chunk 530\n",
      "Processing chunk 531\n",
      "Finished processing chunk 531\n",
      "Processing chunk 532\n",
      "Finished processing chunk 532\n",
      "Processing chunk 533\n",
      "Finished processing chunk 533\n",
      "Processing chunk 534\n",
      "Finished processing chunk 534\n",
      "Processing chunk 535\n",
      "Finished processing chunk 535\n",
      "Processing chunk 536\n",
      "Finished processing chunk 536\n",
      "Processing chunk 537\n",
      "Finished processing chunk 537\n",
      "Processing chunk 538\n",
      "Finished processing chunk 538\n",
      "Processing chunk 539\n",
      "Finished processing chunk 539\n",
      "Processing chunk 540\n",
      "Finished processing chunk 540\n",
      "Processing chunk 541\n",
      "Finished processing chunk 541\n",
      "Processing chunk 542\n",
      "Finished processing chunk 542\n",
      "Processing chunk 543\n",
      "Finished processing chunk 543\n",
      "Processing chunk 544\n",
      "Finished processing chunk 544\n",
      "Processing chunk 545\n",
      "Finished processing chunk 545\n",
      "Processing chunk 546\n",
      "Finished processing chunk 546\n",
      "Processing chunk 547\n",
      "Finished processing chunk 547\n",
      "Processing chunk 548\n",
      "Finished processing chunk 548\n",
      "Processing chunk 549\n",
      "Finished processing chunk 549\n",
      "Processing chunk 550\n",
      "Finished processing chunk 550\n",
      "Processing chunk 551\n",
      "Finished processing chunk 551\n",
      "Processing chunk 552\n",
      "Finished processing chunk 552\n",
      "Processing chunk 553\n",
      "Finished processing chunk 553\n",
      "Processing chunk 554\n",
      "Finished processing chunk 554\n",
      "Processing chunk 555\n",
      "Finished processing chunk 555\n",
      "Processing chunk 556\n",
      "Finished processing chunk 556\n",
      "Processing chunk 557\n",
      "Finished processing chunk 557\n",
      "Processing chunk 558\n",
      "Finished processing chunk 558\n",
      "Processing chunk 559\n",
      "Finished processing chunk 559\n",
      "Processing chunk 560\n",
      "Finished processing chunk 560\n",
      "Processing chunk 561\n",
      "Finished processing chunk 561\n",
      "Processing chunk 562\n",
      "Finished processing chunk 562\n",
      "Processing chunk 563\n",
      "Finished processing chunk 563\n",
      "Processing chunk 564\n",
      "Finished processing chunk 564\n",
      "Processing chunk 565\n",
      "Finished processing chunk 565\n",
      "Processing chunk 566\n",
      "Finished processing chunk 566\n",
      "Processing chunk 567\n",
      "Finished processing chunk 567\n",
      "Processing chunk 568\n",
      "Finished processing chunk 568\n",
      "Processing chunk 569\n",
      "Finished processing chunk 569\n",
      "Processing chunk 570\n",
      "Finished processing chunk 570\n",
      "Processing chunk 571\n",
      "Finished processing chunk 571\n",
      "Processing chunk 572\n",
      "Finished processing chunk 572\n",
      "Processing chunk 573\n",
      "Finished processing chunk 573\n",
      "Processing chunk 574\n",
      "Finished processing chunk 574\n",
      "Processing chunk 575\n",
      "Finished processing chunk 575\n",
      "Processing chunk 576\n",
      "Finished processing chunk 576\n",
      "Processing chunk 577\n",
      "Finished processing chunk 577\n",
      "Processing chunk 578\n",
      "Finished processing chunk 578\n",
      "Processing chunk 579\n",
      "Finished processing chunk 579\n",
      "Processing chunk 580\n",
      "Finished processing chunk 580\n",
      "Processing chunk 581\n",
      "Finished processing chunk 581\n",
      "Processing chunk 582\n",
      "Finished processing chunk 582\n",
      "Processing chunk 583\n",
      "Finished processing chunk 583\n",
      "Processing chunk 584\n",
      "Finished processing chunk 584\n",
      "Processing chunk 585\n",
      "Finished processing chunk 585\n",
      "Processing chunk 586\n",
      "Finished processing chunk 586\n",
      "Processing chunk 587\n",
      "Finished processing chunk 587\n",
      "Processing chunk 588\n",
      "Finished processing chunk 588\n",
      "Processing chunk 589\n",
      "Finished processing chunk 589\n",
      "Processing chunk 590\n",
      "Finished processing chunk 590\n",
      "Processing chunk 591\n",
      "Finished processing chunk 591\n",
      "Processing chunk 592\n",
      "Finished processing chunk 592\n",
      "Processing chunk 593\n",
      "Finished processing chunk 593\n",
      "Processing chunk 594\n",
      "Finished processing chunk 594\n",
      "Processing chunk 595\n",
      "Finished processing chunk 595\n",
      "Processing chunk 596\n",
      "Finished processing chunk 596\n",
      "Processing chunk 597\n",
      "Finished processing chunk 597\n",
      "Processing chunk 598\n",
      "Finished processing chunk 598\n",
      "Processing chunk 599\n",
      "Finished processing chunk 599\n",
      "Processing chunk 600\n",
      "Finished processing chunk 600\n",
      "Processing chunk 601\n",
      "Finished processing chunk 601\n",
      "Processing chunk 602\n",
      "Finished processing chunk 602\n",
      "Processing chunk 603\n",
      "Finished processing chunk 603\n",
      "Processing chunk 604\n",
      "Finished processing chunk 604\n",
      "Processing chunk 605\n",
      "Finished processing chunk 605\n",
      "Processing chunk 606\n",
      "Finished processing chunk 606\n",
      "Processing chunk 607\n",
      "Finished processing chunk 607\n",
      "Processing chunk 608\n",
      "Finished processing chunk 608\n",
      "Processing chunk 609\n",
      "Finished processing chunk 609\n",
      "Processing chunk 610\n",
      "Finished processing chunk 610\n",
      "Processing chunk 611\n",
      "Finished processing chunk 611\n",
      "Processing chunk 612\n",
      "Finished processing chunk 612\n",
      "Processing chunk 613\n",
      "Finished processing chunk 613\n",
      "Processing chunk 614\n",
      "Finished processing chunk 614\n",
      "Processing chunk 615\n",
      "Finished processing chunk 615\n",
      "Processing chunk 616\n",
      "Finished processing chunk 616\n",
      "Processing chunk 617\n",
      "Finished processing chunk 617\n",
      "Processing chunk 618\n",
      "Finished processing chunk 618\n",
      "Processing chunk 619\n",
      "Finished processing chunk 619\n",
      "Processing chunk 620\n",
      "Finished processing chunk 620\n",
      "Processing chunk 621\n",
      "Finished processing chunk 621\n",
      "Processing chunk 622\n",
      "Finished processing chunk 622\n",
      "Processing chunk 623\n",
      "Finished processing chunk 623\n",
      "Processing chunk 624\n",
      "Finished processing chunk 624\n",
      "Processing chunk 625\n",
      "Finished processing chunk 625\n",
      "Processing chunk 626\n",
      "Finished processing chunk 626\n",
      "Processing chunk 627\n",
      "Finished processing chunk 627\n",
      "Processing chunk 628\n",
      "Finished processing chunk 628\n",
      "Processing chunk 629\n",
      "Finished processing chunk 629\n",
      "Processing chunk 630\n",
      "Finished processing chunk 630\n",
      "Processing chunk 631\n",
      "Finished processing chunk 631\n",
      "Processing chunk 632\n",
      "Finished processing chunk 632\n",
      "Processing chunk 633\n",
      "Finished processing chunk 633\n",
      "Processing chunk 634\n",
      "Finished processing chunk 634\n",
      "Processing chunk 635\n",
      "Finished processing chunk 635\n",
      "Processing chunk 636\n",
      "Finished processing chunk 636\n",
      "Processing chunk 637\n",
      "Finished processing chunk 637\n",
      "Processing chunk 638\n",
      "Finished processing chunk 638\n",
      "Processing chunk 639\n",
      "Finished processing chunk 639\n",
      "Processing chunk 640\n",
      "Finished processing chunk 640\n",
      "Processing chunk 641\n",
      "Finished processing chunk 641\n",
      "Processing chunk 642\n",
      "Finished processing chunk 642\n",
      "Processing chunk 643\n",
      "Finished processing chunk 643\n",
      "Processing chunk 644\n",
      "Finished processing chunk 644\n",
      "Processing chunk 645\n",
      "Finished processing chunk 645\n",
      "Processing chunk 646\n",
      "Finished processing chunk 646\n",
      "Processing chunk 647\n",
      "Finished processing chunk 647\n",
      "Processing chunk 648\n",
      "Finished processing chunk 648\n",
      "Processing chunk 649\n",
      "Finished processing chunk 649\n",
      "Processing chunk 650\n",
      "Finished processing chunk 650\n",
      "Processing chunk 651\n",
      "Finished processing chunk 651\n",
      "Processing chunk 652\n",
      "Finished processing chunk 652\n",
      "Processing chunk 653\n",
      "Finished processing chunk 653\n",
      "Processing chunk 654\n",
      "Finished processing chunk 654\n",
      "Processing chunk 655\n",
      "Finished processing chunk 655\n",
      "Processing chunk 656\n",
      "Finished processing chunk 656\n",
      "Processing chunk 657\n",
      "Finished processing chunk 657\n",
      "Processing chunk 658\n",
      "Finished processing chunk 658\n",
      "Processing chunk 659\n",
      "Finished processing chunk 659\n",
      "Processing chunk 660\n",
      "Finished processing chunk 660\n",
      "Processing chunk 661\n",
      "Finished processing chunk 661\n",
      "Processing chunk 662\n",
      "Finished processing chunk 662\n",
      "Processing chunk 663\n",
      "Finished processing chunk 663\n",
      "Processing chunk 664\n",
      "Finished processing chunk 664\n",
      "Processing chunk 665\n",
      "Finished processing chunk 665\n",
      "Processing chunk 666\n",
      "Finished processing chunk 666\n",
      "Processing chunk 667\n",
      "Finished processing chunk 667\n",
      "Processing chunk 668\n",
      "Finished processing chunk 668\n",
      "Processing chunk 669\n",
      "Finished processing chunk 669\n",
      "Processing chunk 670\n",
      "Finished processing chunk 670\n",
      "Processing chunk 671\n",
      "Finished processing chunk 671\n",
      "Processing chunk 672\n",
      "Finished processing chunk 672\n",
      "Processing chunk 673\n",
      "Finished processing chunk 673\n",
      "Processing chunk 674\n",
      "Finished processing chunk 674\n",
      "Processing chunk 675\n",
      "Finished processing chunk 675\n",
      "Processing chunk 676\n",
      "Finished processing chunk 676\n",
      "Processing chunk 677\n",
      "Finished processing chunk 677\n",
      "Processing chunk 678\n",
      "Finished processing chunk 678\n",
      "Processing chunk 679\n",
      "Finished processing chunk 679\n",
      "Processing chunk 680\n",
      "Finished processing chunk 680\n",
      "Processing chunk 681\n",
      "Finished processing chunk 681\n",
      "Processing chunk 682\n",
      "Finished processing chunk 682\n",
      "Processing chunk 683\n",
      "Finished processing chunk 683\n",
      "Processing chunk 684\n",
      "Finished processing chunk 684\n",
      "Processing chunk 685\n",
      "Finished processing chunk 685\n",
      "Processing chunk 686\n",
      "Finished processing chunk 686\n",
      "Processing chunk 687\n",
      "Finished processing chunk 687\n",
      "Processing chunk 688\n",
      "Finished processing chunk 688\n",
      "Processing chunk 689\n",
      "Finished processing chunk 689\n",
      "Processing chunk 690\n",
      "Finished processing chunk 690\n",
      "Processing chunk 691\n",
      "Finished processing chunk 691\n",
      "Processing chunk 692\n",
      "Finished processing chunk 692\n",
      "Processing chunk 693\n",
      "Finished processing chunk 693\n",
      "Processing chunk 694\n",
      "Finished processing chunk 694\n",
      "Processing chunk 695\n",
      "Finished processing chunk 695\n",
      "Processing chunk 696\n",
      "Finished processing chunk 696\n",
      "Processing chunk 697\n",
      "Finished processing chunk 697\n",
      "Processing chunk 698\n",
      "Finished processing chunk 698\n",
      "Processing chunk 699\n",
      "Finished processing chunk 699\n",
      "Processing chunk 700\n",
      "Finished processing chunk 700\n",
      "Processing chunk 701\n",
      "Finished processing chunk 701\n",
      "Processing chunk 702\n",
      "Finished processing chunk 702\n",
      "Processing chunk 703\n",
      "Finished processing chunk 703\n",
      "Processing chunk 704\n",
      "Finished processing chunk 704\n",
      "All chunks have been processed\n"
     ]
    }
   ],
   "source": [
    "df_mm = pd.read_csv('Data/SharedResponses.csv', chunksize=100_000, dtype=str, low_memory=False)\n",
    "\n",
    "cleaned_chunks = []\n",
    "for i, chunk in enumerate(df_mm):\n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "    \n",
    "    # Cleaning \n",
    "    chunk_cleaned = chunk[chunk['ExtendedSessionID'].isin(ids_to_keep)]\n",
    "    chunk_cleaned = chunk_cleaned.drop(columns=['ScenarioOrder', 'Intervention', 'ScenarioType', 'DefaultChoice', 'NonDefaultChoice', 'DefaultChoiceIsOmission', 'Template','DescriptionShown','LeftHand'])\n",
    "    chunk_cleaned.rename(columns={'UserCountry3': 'Country'}, inplace=True)\n",
    "    chunk_cleaned = chunk_cleaned.dropna()\n",
    "    chunk_cleaned = chunk_cleaned.drop_duplicates()\n",
    "    chunk_cleaned = chunk_cleaned.merge(df_tri_cleaned, on='Country', how='inner').merge(df_rtds, on='Country', how='inner').merge(df_rtdu_cleaned, on='Country', how='inner')\n",
    "    chunk_cleaned[['ResponseID', 'ExtendedSessionID', 'UserID', 'AttributeLevel','ScenarioTypeStrict']] = chunk_cleaned[['ResponseID', 'ExtendedSessionID', 'UserID', 'AttributeLevel','ScenarioTypeStrict']].astype(str)\n",
    "    chunk_cleaned[[\"PedPed\", \"Barrier\", \"CrossingSignal\", \"NumberOfCharacters\", \"DiffNumberOFCharacters\", \"Man\", \"Woman\", \"Pregnant\", \"Stroller\", \"OldMan\", \"OldWoman\", \"Boy\", \"Girl\", \"Homeless\", \"LargeWoman\", \"LargeMan\", \"Criminal\", \"MaleExecutive\", \"FemaleExecutive\", \"FemaleAthlete\", \"MaleAthlete\", \"FemaleDoctor\", \"MaleDoctor\", \"Dog\", \"Cat\", \"Saved\"]] = chunk_cleaned[[\"PedPed\", \"Barrier\", \"CrossingSignal\", \"NumberOfCharacters\", \"DiffNumberOFCharacters\", \"Man\", \"Woman\", \"Pregnant\", \"Stroller\", \"OldMan\", \"OldWoman\", \"Boy\", \"Girl\", \"Homeless\", \"LargeWoman\", \"LargeMan\", \"Criminal\", \"MaleExecutive\", \"FemaleExecutive\", \"FemaleAthlete\", \"MaleAthlete\", \"FemaleDoctor\", \"MaleDoctor\", \"Dog\", \"Cat\", \"Saved\"]].astype(float).round().astype('int8')\n",
    "    chunk_cleaned[[\"Finance_access\", \"ICT\", \"Industry_activity\", \"Overall_index\", \"Research_and_development\", \"Skills\", \"Total\", \"Males\", \"Females\", \"Passengers\", \"Pedestrians\"]] = chunk_cleaned[[\"Finance_access\", \"ICT\", \"Industry_activity\", \"Overall_index\", \"Research_and_development\", \"Skills\", \"Total\", \"Males\", \"Females\", \"Passengers\", \"Pedestrians\"]].astype('float32')\n",
    "\n",
    "    # Append cleaned chunk\n",
    "    cleaned_chunks.append(chunk_cleaned)\n",
    "    print(f\"Finished processing chunk {i+1}\")\n",
    "\n",
    "    # Stop after processing five chunks\n",
    "    #j = 5\n",
    "    #if i + 1 == j:\n",
    "    #    print(f\"Stopping after processing {j} chunks\")\n",
    "    #    break\n",
    "\n",
    "print(\"All chunks have been processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.concat(cleaned_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseid_counts = cleaned_data['ResponseID'].value_counts()\n",
    "responseid_to_keep = responseid_counts[responseid_counts == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data[cleaned_data['ResponseID'].isin(responseid_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "412256\n"
     ]
    }
   ],
   "source": [
    "# Get the value counts for each ExtendedSessionID\n",
    "session_counts = cleaned_data['ExtendedSessionID'].value_counts()\n",
    "\n",
    "# Step 1: Calculate the counts for 26, 25, and 24 occurrences\n",
    "count_26 = (session_counts == 26).sum()\n",
    "count_25 = (session_counts == 25).sum()\n",
    "count_24 = (session_counts == 24).sum()\n",
    "\n",
    "# Step 2: Set the target number of rows (10 million)\n",
    "target_rows = 10_000_000\n",
    "\n",
    "# Step 3: Calculate the number of rows if we keep all ExtendedSessionIDs appearing 26 times\n",
    "rows_from_26 = count_26 * 26\n",
    "\n",
    "# Step 4: If keeping all ExtendedSessionIDs appearing 26 times keeps us under the target:\n",
    "if rows_from_26 <= target_rows:\n",
    "    # Keep all ExtendedSessionIDs appearing 26 times\n",
    "    ids_to_keep = session_counts[session_counts == 26].index\n",
    "    remaining_rows = target_rows - rows_from_26\n",
    "    \n",
    "    # Step 5: Calculate the number of rows if we also keep all ExtendedSessionIDs appearing 25 times\n",
    "    rows_from_25 = count_25 * 25\n",
    "    if rows_from_25 <= remaining_rows:\n",
    "        # Keep all ExtendedSessionIDs appearing 25 times\n",
    "        ids_to_keep = ids_to_keep.union(session_counts[session_counts == 25].index)\n",
    "        remaining_rows -= rows_from_25\n",
    "        \n",
    "        # Step 6: Calculate the number of rows if we also keep all ExtendedSessionIDs appearing 24 times\n",
    "        rows_from_24 = count_24 * 24\n",
    "        if rows_from_24 <= remaining_rows:\n",
    "            # Keep all ExtendedSessionIDs appearing 24 times\n",
    "            ids_to_keep = ids_to_keep.union(session_counts[session_counts == 24].index)\n",
    "        else:\n",
    "            # Keep only enough ExtendedSessionIDs appearing 24 times to reach the target\n",
    "            num_to_keep_24 = remaining_rows // 24\n",
    "            ids_to_keep = ids_to_keep.union(session_counts[session_counts == 24].index[:num_to_keep_24])\n",
    "    else:\n",
    "        # Keep only enough ExtendedSessionIDs appearing 25 times to reach the target\n",
    "        num_to_keep_25 = remaining_rows // 25\n",
    "        ids_to_keep = ids_to_keep.union(session_counts[session_counts == 25].index[:num_to_keep_25])\n",
    "else:\n",
    "    # Keep only enough ExtendedSessionIDs appearing 26 times to reach the target\n",
    "    num_to_keep_26 = target_rows // 26\n",
    "    ids_to_keep = session_counts[session_counts == 26].index[:num_to_keep_26]\n",
    "\n",
    "# Step 7: Filter the original dataset to keep only the selected ExtendedSessionIDs\n",
    "filtered_sessions = extendedsessionid[extendedsessionid['ExtendedSessionID'].isin(ids_to_keep)]\n",
    "filtered_sessions = (filtered_sessions['ExtendedSessionID'].unique()).tolist()\n",
    "print(type(filtered_sessions))\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(len(filtered_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data[cleaned_data['ExtendedSessionID'].isin(filtered_sessions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>NumberOfCharacters</th>\n",
       "      <th>DiffNumberOFCharacters</th>\n",
       "      <th>Saved</th>\n",
       "      <th>Country</th>\n",
       "      <th>Man</th>\n",
       "      <th>Woman</th>\n",
       "      <th>Pregnant</th>\n",
       "      <th>Stroller</th>\n",
       "      <th>OldMan</th>\n",
       "      <th>OldWoman</th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>Homeless</th>\n",
       "      <th>LargeWoman</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Finance_access</th>\n",
       "      <th>ICT</th>\n",
       "      <th>Industry_activity</th>\n",
       "      <th>Overall_index</th>\n",
       "      <th>Research_and_development</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Total</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Pedestrians</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2223Xu54ufgjcyMR3</td>\n",
       "      <td>1425316635_327833569077076.0</td>\n",
       "      <td>327833569077076</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Old</td>\n",
       "      <td>Age</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEX</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13.60</td>\n",
       "      <td>22.049999</td>\n",
       "      <td>5.45</td>\n",
       "      <td>2.5024</td>\n",
       "      <td>3.8760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2223jMWDEGNeszivb</td>\n",
       "      <td>-1683127088_785070916172117.0</td>\n",
       "      <td>785070916172117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>CHE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.90</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>0.6237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>222HpiEf2LtAwEg62</td>\n",
       "      <td>-1232628507_1597557389</td>\n",
       "      <td>1597557389</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>UKR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13.25</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.4120</td>\n",
       "      <td>5.6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>222KuWty7pNeiv77a</td>\n",
       "      <td>1654911454_3639764894860440.0</td>\n",
       "      <td>3639764894860440</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12.50</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>7.25</td>\n",
       "      <td>3.9603</td>\n",
       "      <td>1.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>222LDp4wz24C3chzj</td>\n",
       "      <td>-1679158262_3623236506.0</td>\n",
       "      <td>3623236506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fat</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DEU</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.20</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.9120</td>\n",
       "      <td>0.6120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>222dZwp7jYt7FrkfQ</td>\n",
       "      <td>781757349_7305361930957958.0</td>\n",
       "      <td>7305361930957960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12.50</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>7.25</td>\n",
       "      <td>3.9603</td>\n",
       "      <td>1.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>222fkCAzoe6MAnMsP</td>\n",
       "      <td>-624226515_2260272466.0</td>\n",
       "      <td>2260272466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NLD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.80</td>\n",
       "      <td>5.650000</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.4440</td>\n",
       "      <td>0.3496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>222t9Qtkcc4EnQvcc</td>\n",
       "      <td>1829270983_2465676825.0</td>\n",
       "      <td>2465676825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CAN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.7294</td>\n",
       "      <td>0.8816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>223BYifT53tQSv7Yg</td>\n",
       "      <td>-1594341876_1841400032020538.0</td>\n",
       "      <td>1841400032020540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Young</td>\n",
       "      <td>Age</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12.50</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>7.25</td>\n",
       "      <td>3.9603</td>\n",
       "      <td>1.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>223NQzNqcbab7XWQn</td>\n",
       "      <td>-409747588_5976174606267197.0</td>\n",
       "      <td>5976174606267200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AZE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>9.50</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.5066</td>\n",
       "      <td>3.6540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ResponseID               ExtendedSessionID            UserID  \\\n",
       "0    2223Xu54ufgjcyMR3    1425316635_327833569077076.0   327833569077076   \n",
       "1    2223jMWDEGNeszivb   -1683127088_785070916172117.0   785070916172117   \n",
       "25   222HpiEf2LtAwEg62          -1232628507_1597557389        1597557389   \n",
       "28   222KuWty7pNeiv77a   1654911454_3639764894860440.0  3639764894860440   \n",
       "30   222LDp4wz24C3chzj        -1679158262_3623236506.0        3623236506   \n",
       "51   222dZwp7jYt7FrkfQ    781757349_7305361930957958.0  7305361930957960   \n",
       "55   222fkCAzoe6MAnMsP         -624226515_2260272466.0        2260272466   \n",
       "70   222t9Qtkcc4EnQvcc         1829270983_2465676825.0        2465676825   \n",
       "90   223BYifT53tQSv7Yg  -1594341876_1841400032020538.0  1841400032020540   \n",
       "105  223NQzNqcbab7XWQn   -409747588_5976174606267197.0  5976174606267200   \n",
       "\n",
       "     PedPed  Barrier  CrossingSignal AttributeLevel ScenarioTypeStrict  \\\n",
       "0         0        1               0            Old                Age   \n",
       "1         1        0               2           More        Utilitarian   \n",
       "25        0        1               0         Female             Gender   \n",
       "28        1        0               0            Low      Social Status   \n",
       "30        0        0               0            Fat            Fitness   \n",
       "51        1        0               0           Male             Gender   \n",
       "55        0        0               0         Female             Gender   \n",
       "70        1        0               0           More        Utilitarian   \n",
       "90        1        0               2          Young                Age   \n",
       "105       1        0               1           More        Utilitarian   \n",
       "\n",
       "     NumberOfCharacters  DiffNumberOFCharacters  Saved Country  Man  Woman  \\\n",
       "0                     5                       0      0     MEX    0      0   \n",
       "1                     5                       2      0     CHE    0      0   \n",
       "25                    2                       0      0     UKR    0      0   \n",
       "28                    2                       0      0     USA    0      0   \n",
       "30                    2                       0      0     DEU    1      0   \n",
       "51                    1                       0      0     USA    0      0   \n",
       "55                    1                       0      1     NLD    0      0   \n",
       "70                    5                       2      1     CAN    0      0   \n",
       "90                    3                       0      0     USA    1      0   \n",
       "105                   5                       1      1     AZE    0      0   \n",
       "\n",
       "     Pregnant  Stroller  OldMan  OldWoman  Boy  Girl  Homeless  LargeWoman  \\\n",
       "0           0         0       2         3    0     0         0           0   \n",
       "1           0         0       1         0    0     0         0           0   \n",
       "25          0         0       0         1    0     0         0           1   \n",
       "28          0         0       0         0    0     0         2           0   \n",
       "30          0         0       0         0    0     0         0           1   \n",
       "51          0         0       1         0    0     0         0           0   \n",
       "55          0         0       0         0    0     0         0           0   \n",
       "70          0         0       1         1    0     0         0           0   \n",
       "90          0         0       0         0    1     1         0           0   \n",
       "105         0         1       0         0    0     0         1           0   \n",
       "\n",
       "     LargeMan  Criminal  MaleExecutive  FemaleExecutive  FemaleAthlete  \\\n",
       "0           0         0              0                0              0   \n",
       "1           0         1              1                0              1   \n",
       "25          0         0              0                0              0   \n",
       "28          0         0              0                0              0   \n",
       "30          0         0              0                0              0   \n",
       "51          0         0              0                0              0   \n",
       "55          0         0              0                1              0   \n",
       "70          0         0              0                0              2   \n",
       "90          0         0              0                0              0   \n",
       "105         0         2              0                1              0   \n",
       "\n",
       "     MaleAthlete  FemaleDoctor  MaleDoctor  Dog  Cat  Finance_access   ICT  \\\n",
       "0              0             0           0    0    0            0.60  0.55   \n",
       "1              0             0           0    1    0            0.90  0.65   \n",
       "25             0             0           0    0    0            0.75  0.50   \n",
       "28             0             0           0    0    0            0.90  0.65   \n",
       "30             0             0           0    0    0            0.80  0.80   \n",
       "51             0             0           0    0    0            0.90  0.65   \n",
       "55             0             0           0    0    0            0.80  0.80   \n",
       "70             0             0           0    0    1            0.90  0.80   \n",
       "90             0             0           0    0    0            0.90  0.65   \n",
       "105            0             0           0    0    0            0.60  0.50   \n",
       "\n",
       "     Industry_activity  Overall_index  Research_and_development  Skills  \\\n",
       "0                 0.80           0.60                       0.5    0.40   \n",
       "1                 0.90           0.90                       0.7    0.80   \n",
       "25                0.65           0.65                       0.5    0.65   \n",
       "28                0.80           1.00                       1.0    0.75   \n",
       "30                0.90           0.90                       0.8    0.75   \n",
       "51                0.80           1.00                       1.0    0.75   \n",
       "55                0.90           0.95                       0.7    0.85   \n",
       "70                0.80           0.90                       0.7    0.75   \n",
       "90                0.80           1.00                       1.0    0.75   \n",
       "105               0.35           0.30                       0.1    0.35   \n",
       "\n",
       "     Total      Males  Females  Passengers  Pedestrians  \n",
       "0    13.60  22.049999     5.45      2.5024       3.8760  \n",
       "1     2.90   4.200000     1.60      0.5076       0.6237  \n",
       "25   13.25  21.750000     5.95      7.4120       5.6984  \n",
       "28   12.50  17.850000     7.25      3.9603       1.9737  \n",
       "30    4.20   6.350000     2.15      1.9120       0.6120  \n",
       "51   12.50  17.850000     7.25      3.9603       1.9737  \n",
       "55    3.80   5.650000     1.95      1.4440       0.3496  \n",
       "70    5.80   8.150000     3.45      3.7294       0.8816  \n",
       "90   12.50  17.850000     7.25      3.9603       1.9737  \n",
       "105   9.50  15.250000     3.80      4.5066       3.6540  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "cleaned_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtendedSessionID\n",
       "-1515635037_7014932942427610.0    26\n",
       "-649792789_123617179              26\n",
       "-1631266300_7001055290384566.0    26\n",
       "-551512359_4144432253.0           26\n",
       "-451394117_519359821219711.0      26\n",
       "                                  ..\n",
       "1196203245_9395865181510246.0     24\n",
       "-417777874_2014408738             24\n",
       "-401464432_2285117361.0           24\n",
       "1656669350_9046809254077914.0     24\n",
       "551980216_3319982808.0            24\n",
       "Name: count, Length: 412256, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extendedsessionid_values = cleaned_data['ExtendedSessionID'].value_counts()\n",
    "extendedsessionid_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52924\n",
      "0\n",
      "359332\n"
     ]
    }
   ],
   "source": [
    "# Number of value_count is 24, 25 and 26\n",
    "count_26 = (extendedsessionid_values == 26).sum()\n",
    "count_25 = (extendedsessionid_values == 25).sum()\n",
    "count_24 = (extendedsessionid_values == 24).sum()\n",
    "print(count_26)\n",
    "print(count_25)\n",
    "print(count_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_once = (extendedsessionid_values < 24).sum()\n",
    "count_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent value: 2223Xu54ufgjcyMR3, Count: 2\n"
     ]
    }
   ],
   "source": [
    "most_frequent_value = cleaned_data['ResponseID'].value_counts().idxmax()\n",
    "max_value_count = cleaned_data['ResponseID'].value_counts().max()\n",
    "least_frequent_value = cleaned_data['ResponseID'].value_counts().idxmin()\n",
    "min_value_count = cleaned_data['ResponseID'].value_counts().min()\n",
    "\n",
    "print(f\"Most frequent value: {most_frequent_value}, Count: {max_value_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent value: 2223Xu54ufgjcyMR3, Count: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Least frequent value: {least_frequent_value}, Count: {min_value_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>NumberOfCharacters</th>\n",
       "      <th>DiffNumberOFCharacters</th>\n",
       "      <th>Saved</th>\n",
       "      <th>Man</th>\n",
       "      <th>Woman</th>\n",
       "      <th>Pregnant</th>\n",
       "      <th>Stroller</th>\n",
       "      <th>OldMan</th>\n",
       "      <th>OldWoman</th>\n",
       "      <th>Boy</th>\n",
       "      <th>Girl</th>\n",
       "      <th>Homeless</th>\n",
       "      <th>LargeWoman</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Finance_access</th>\n",
       "      <th>ICT</th>\n",
       "      <th>Industry_activity</th>\n",
       "      <th>Overall_index</th>\n",
       "      <th>Research_and_development</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Total</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Pedestrians</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9999992.0</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "      <td>9.999992e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.517082e-01</td>\n",
       "      <td>2.741459e-01</td>\n",
       "      <td>6.061092e-01</td>\n",
       "      <td>2.990266e+00</td>\n",
       "      <td>5.458598e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.102753e-01</td>\n",
       "      <td>3.106293e-01</td>\n",
       "      <td>5.972915e-02</td>\n",
       "      <td>5.703515e-02</td>\n",
       "      <td>1.850920e-01</td>\n",
       "      <td>1.852236e-01</td>\n",
       "      <td>1.541063e-01</td>\n",
       "      <td>1.541007e-01</td>\n",
       "      <td>1.050380e-01</td>\n",
       "      <td>1.541672e-01</td>\n",
       "      <td>1.540358e-01</td>\n",
       "      <td>5.683015e-02</td>\n",
       "      <td>1.052104e-01</td>\n",
       "      <td>1.053253e-01</td>\n",
       "      <td>1.849366e-01</td>\n",
       "      <td>1.847790e-01</td>\n",
       "      <td>9.488708e-02</td>\n",
       "      <td>9.502938e-02</td>\n",
       "      <td>1.667987e-01</td>\n",
       "      <td>1.670362e-01</td>\n",
       "      <td>8.163826e-01</td>\n",
       "      <td>6.750644e-01</td>\n",
       "      <td>7.947744e-01</td>\n",
       "      <td>8.634647e-01</td>\n",
       "      <td>7.364607e-01</td>\n",
       "      <td>7.264218e-01</td>\n",
       "      <td>9.046264e+00</td>\n",
       "      <td>1.368448e+01</td>\n",
       "      <td>4.554417e+00</td>\n",
       "      <td>3.267029e+00</td>\n",
       "      <td>1.656296e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.976625e-01</td>\n",
       "      <td>4.460829e-01</td>\n",
       "      <td>8.147176e-01</td>\n",
       "      <td>1.482857e+00</td>\n",
       "      <td>1.129296e+00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.908965e-01</td>\n",
       "      <td>5.913108e-01</td>\n",
       "      <td>2.556587e-01</td>\n",
       "      <td>2.502378e-01</td>\n",
       "      <td>5.156442e-01</td>\n",
       "      <td>5.156233e-01</td>\n",
       "      <td>4.404961e-01</td>\n",
       "      <td>4.402197e-01</td>\n",
       "      <td>3.794025e-01</td>\n",
       "      <td>4.403530e-01</td>\n",
       "      <td>4.400539e-01</td>\n",
       "      <td>2.497549e-01</td>\n",
       "      <td>3.452981e-01</td>\n",
       "      <td>3.452887e-01</td>\n",
       "      <td>5.154392e-01</td>\n",
       "      <td>5.151208e-01</td>\n",
       "      <td>3.291838e-01</td>\n",
       "      <td>3.295424e-01</td>\n",
       "      <td>5.566472e-01</td>\n",
       "      <td>5.579227e-01</td>\n",
       "      <td>1.036725e-01</td>\n",
       "      <td>1.253239e-01</td>\n",
       "      <td>8.401662e-02</td>\n",
       "      <td>1.371994e-01</td>\n",
       "      <td>2.141902e-01</td>\n",
       "      <td>1.230905e-01</td>\n",
       "      <td>5.086800e+00</td>\n",
       "      <td>8.133916e+00</td>\n",
       "      <td>2.369699e+00</td>\n",
       "      <td>1.574745e+00</td>\n",
       "      <td>1.096867e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.750000e+00</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.220000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.500000e-01</td>\n",
       "      <td>6.500000e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>6.500000e-01</td>\n",
       "      <td>4.900000e+00</td>\n",
       "      <td>7.500000e+00</td>\n",
       "      <td>2.450000e+00</td>\n",
       "      <td>1.932000e+00</td>\n",
       "      <td>7.840000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.500000e-01</td>\n",
       "      <td>6.500000e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>7.500000e-01</td>\n",
       "      <td>7.500000e-01</td>\n",
       "      <td>7.800000e+00</td>\n",
       "      <td>1.240000e+01</td>\n",
       "      <td>3.550000e+00</td>\n",
       "      <td>3.574200e+00</td>\n",
       "      <td>1.435000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.500000e-01</td>\n",
       "      <td>1.250000e+01</td>\n",
       "      <td>1.785000e+01</td>\n",
       "      <td>7.250000e+00</td>\n",
       "      <td>3.960300e+00</td>\n",
       "      <td>1.973700e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.415000e+01</td>\n",
       "      <td>5.790000e+01</td>\n",
       "      <td>2.390000e+01</td>\n",
       "      <td>1.955340e+01</td>\n",
       "      <td>1.749030e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PedPed       Barrier  CrossingSignal  NumberOfCharacters  \\\n",
       "count  9.999992e+06  9.999992e+06    9.999992e+06        9.999992e+06   \n",
       "mean   4.517082e-01  2.741459e-01    6.061092e-01        2.990266e+00   \n",
       "std    4.976625e-01  4.460829e-01    8.147176e-01        1.482857e+00   \n",
       "min    0.000000e+00  0.000000e+00    0.000000e+00        1.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00    0.000000e+00        2.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00    0.000000e+00        3.000000e+00   \n",
       "75%    1.000000e+00  1.000000e+00    1.000000e+00        4.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00    2.000000e+00        5.000000e+00   \n",
       "\n",
       "       DiffNumberOFCharacters      Saved           Man         Woman  \\\n",
       "count            9.999992e+06  9999992.0  9.999992e+06  9.999992e+06   \n",
       "mean             5.458598e-01        0.5  3.102753e-01  3.106293e-01   \n",
       "std              1.129296e+00        0.5  5.908965e-01  5.913108e-01   \n",
       "min              0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "25%              0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "50%              0.000000e+00        0.5  0.000000e+00  0.000000e+00   \n",
       "75%              0.000000e+00        1.0  1.000000e+00  1.000000e+00   \n",
       "max              4.000000e+00        1.0  5.000000e+00  5.000000e+00   \n",
       "\n",
       "           Pregnant      Stroller        OldMan      OldWoman           Boy  \\\n",
       "count  9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06   \n",
       "mean   5.972915e-02  5.703515e-02  1.850920e-01  1.852236e-01  1.541063e-01   \n",
       "std    2.556587e-01  2.502378e-01  5.156442e-01  5.156233e-01  4.404961e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00   \n",
       "\n",
       "               Girl      Homeless    LargeWoman      LargeMan      Criminal  \\\n",
       "count  9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06   \n",
       "mean   1.541007e-01  1.050380e-01  1.541672e-01  1.540358e-01  5.683015e-02   \n",
       "std    4.402197e-01  3.794025e-01  4.403530e-01  4.400539e-01  2.497549e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00   \n",
       "\n",
       "       MaleExecutive  FemaleExecutive  FemaleAthlete   MaleAthlete  \\\n",
       "count   9.999992e+06     9.999992e+06   9.999992e+06  9.999992e+06   \n",
       "mean    1.052104e-01     1.053253e-01   1.849366e-01  1.847790e-01   \n",
       "std     3.452981e-01     3.452887e-01   5.154392e-01  5.151208e-01   \n",
       "min     0.000000e+00     0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "25%     0.000000e+00     0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "50%     0.000000e+00     0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "75%     0.000000e+00     0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "max     5.000000e+00     5.000000e+00   5.000000e+00  5.000000e+00   \n",
       "\n",
       "       FemaleDoctor    MaleDoctor           Dog           Cat  Finance_access  \\\n",
       "count  9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06    9.999992e+06   \n",
       "mean   9.488708e-02  9.502938e-02  1.667987e-01  1.670362e-01    8.163826e-01   \n",
       "std    3.291838e-01  3.295424e-01  5.566472e-01  5.579227e-01    1.036725e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    7.500000e-01   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    8.500000e-01   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    9.000000e-01   \n",
       "max    5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00    1.000000e+00   \n",
       "\n",
       "                ICT  Industry_activity  Overall_index  \\\n",
       "count  9.999992e+06       9.999992e+06   9.999992e+06   \n",
       "mean   6.750644e-01       7.947744e-01   8.634647e-01   \n",
       "std    1.253239e-01       8.401662e-02   1.371994e-01   \n",
       "min    5.000000e-02       2.500000e-01   0.000000e+00   \n",
       "25%    6.500000e-01       8.000000e-01   8.000000e-01   \n",
       "50%    6.500000e-01       8.000000e-01   9.000000e-01   \n",
       "75%    8.000000e-01       8.000000e-01   1.000000e+00   \n",
       "max    1.000000e+00       1.000000e+00   1.000000e+00   \n",
       "\n",
       "       Research_and_development        Skills         Total         Males  \\\n",
       "count              9.999992e+06  9.999992e+06  9.999992e+06  9.999992e+06   \n",
       "mean               7.364607e-01  7.264218e-01  9.046264e+00  1.368448e+01   \n",
       "std                2.141902e-01  1.230905e-01  5.086800e+00  8.133916e+00   \n",
       "min                0.000000e+00  0.000000e+00  1.750000e+00  2.300000e+00   \n",
       "25%                6.000000e-01  6.500000e-01  4.900000e+00  7.500000e+00   \n",
       "50%                7.500000e-01  7.500000e-01  7.800000e+00  1.240000e+01   \n",
       "75%                1.000000e+00  7.500000e-01  1.250000e+01  1.785000e+01   \n",
       "max                1.000000e+00  1.000000e+00  3.415000e+01  5.790000e+01   \n",
       "\n",
       "            Females    Passengers   Pedestrians  \n",
       "count  9.999992e+06  9.999992e+06  9.999992e+06  \n",
       "mean   4.554417e+00  3.267029e+00  1.656296e+00  \n",
       "std    2.369699e+00  1.574745e+00  1.096867e+00  \n",
       "min    9.000000e-01  0.000000e+00  1.220000e-01  \n",
       "25%    2.450000e+00  1.932000e+00  7.840000e-01  \n",
       "50%    3.550000e+00  3.574200e+00  1.435000e+00  \n",
       "75%    7.250000e+00  3.960300e+00  1.973700e+00  \n",
       "max    2.390000e+01  1.955340e+01  1.749030e+01  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9999992 entries, 0 to 24102222\n",
      "Data columns (total 43 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   ResponseID                object \n",
      " 1   ExtendedSessionID         object \n",
      " 2   UserID                    object \n",
      " 3   PedPed                    int8   \n",
      " 4   Barrier                   int8   \n",
      " 5   CrossingSignal            int8   \n",
      " 6   AttributeLevel            object \n",
      " 7   ScenarioTypeStrict        object \n",
      " 8   NumberOfCharacters        int8   \n",
      " 9   DiffNumberOFCharacters    int8   \n",
      " 10  Saved                     int8   \n",
      " 11  Country                   object \n",
      " 12  Man                       int8   \n",
      " 13  Woman                     int8   \n",
      " 14  Pregnant                  int8   \n",
      " 15  Stroller                  int8   \n",
      " 16  OldMan                    int8   \n",
      " 17  OldWoman                  int8   \n",
      " 18  Boy                       int8   \n",
      " 19  Girl                      int8   \n",
      " 20  Homeless                  int8   \n",
      " 21  LargeWoman                int8   \n",
      " 22  LargeMan                  int8   \n",
      " 23  Criminal                  int8   \n",
      " 24  MaleExecutive             int8   \n",
      " 25  FemaleExecutive           int8   \n",
      " 26  FemaleAthlete             int8   \n",
      " 27  MaleAthlete               int8   \n",
      " 28  FemaleDoctor              int8   \n",
      " 29  MaleDoctor                int8   \n",
      " 30  Dog                       int8   \n",
      " 31  Cat                       int8   \n",
      " 32  Finance_access            float32\n",
      " 33  ICT                       float32\n",
      " 34  Industry_activity         float32\n",
      " 35  Overall_index             float32\n",
      " 36  Research_and_development  float32\n",
      " 37  Skills                    float32\n",
      " 38  Total                     float32\n",
      " 39  Males                     float32\n",
      " 40  Females                   float32\n",
      " 41  Passengers                float32\n",
      " 42  Pedestrians               float32\n",
      "dtypes: float32(11), int8(26), object(6)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "cleaned_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID                  0\n",
       "ExtendedSessionID           0\n",
       "UserID                      0\n",
       "PedPed                      0\n",
       "Barrier                     0\n",
       "CrossingSignal              0\n",
       "AttributeLevel              0\n",
       "ScenarioTypeStrict          0\n",
       "NumberOfCharacters          0\n",
       "DiffNumberOFCharacters      0\n",
       "Saved                       0\n",
       "Country                     0\n",
       "Man                         0\n",
       "Woman                       0\n",
       "Pregnant                    0\n",
       "Stroller                    0\n",
       "OldMan                      0\n",
       "OldWoman                    0\n",
       "Boy                         0\n",
       "Girl                        0\n",
       "Homeless                    0\n",
       "LargeWoman                  0\n",
       "LargeMan                    0\n",
       "Criminal                    0\n",
       "MaleExecutive               0\n",
       "FemaleExecutive             0\n",
       "FemaleAthlete               0\n",
       "MaleAthlete                 0\n",
       "FemaleDoctor                0\n",
       "MaleDoctor                  0\n",
       "Dog                         0\n",
       "Cat                         0\n",
       "Finance_access              0\n",
       "ICT                         0\n",
       "Industry_activity           0\n",
       "Overall_index               0\n",
       "Research_and_development    0\n",
       "Skills                      0\n",
       "Total                       0\n",
       "Males                       0\n",
       "Females                     0\n",
       "Passengers                  0\n",
       "Pedestrians                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = cleaned_data.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ResponseID', 'ExtendedSessionID', 'UserID', 'PedPed', 'Barrier',\n",
       "       'CrossingSignal', 'AttributeLevel', 'ScenarioTypeStrict',\n",
       "       'NumberOfCharacters', 'DiffNumberOFCharacters', 'Saved', 'Country',\n",
       "       'Man', 'Woman', 'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy',\n",
       "       'Girl', 'Homeless', 'LargeWoman', 'LargeMan', 'Criminal',\n",
       "       'MaleExecutive', 'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete',\n",
       "       'FemaleDoctor', 'MaleDoctor', 'Dog', 'Cat', 'Finance_access', 'ICT',\n",
       "       'Industry_activity', 'Overall_index', 'Research_and_development',\n",
       "       'Skills', 'Total', 'Males', 'Females', 'Passengers', 'Pedestrians'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeLevel\n",
      "Rand       11.817409\n",
      "More        8.499467\n",
      "Less        8.499467\n",
      "Pets        8.279137\n",
      "Hoomans     8.279137\n",
      "Female      8.252817\n",
      "Male        8.252817\n",
      "Old         7.977286\n",
      "Young       7.977286\n",
      "Fat         7.444226\n",
      "Fit         7.444226\n",
      "Low         3.638363\n",
      "High        3.638363\n",
      "Name: proportion, dtype: float64\n",
      "ScenarioTypeStrict\n",
      "Utilitarian      16.503373\n",
      "Gender           16.493593\n",
      "Species          16.493513\n",
      "Fitness          16.493493\n",
      "Age              16.492013\n",
      "Social Status     9.280227\n",
      "Random            8.243787\n",
      "Name: proportion, dtype: float64\n",
      "NumberOfCharacters\n",
      "5    23.192669\n",
      "1    22.695198\n",
      "2    19.156355\n",
      "3    17.767794\n",
      "4    17.187984\n",
      "Name: proportion, dtype: float64\n",
      "DiffNumberOFCharacters\n",
      "0    76.877342\n",
      "1     6.752025\n",
      "2     6.092565\n",
      "3     5.463444\n",
      "4     4.814624\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "AttributeLevel_distribution = cleaned_data['AttributeLevel'].value_counts(normalize=True) * 100\n",
    "ScenarioTypeStrict_distribution = cleaned_data['ScenarioTypeStrict'].value_counts(normalize=True) * 100\n",
    "NumberOfCharacters_distribution = cleaned_data['NumberOfCharacters'].value_counts(normalize=True) * 100\n",
    "DiffNumberOFCharacters_distribution = cleaned_data['DiffNumberOFCharacters'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(AttributeLevel_distribution)\n",
    "print(ScenarioTypeStrict_distribution)\n",
    "print(NumberOfCharacters_distribution)\n",
    "print(DiffNumberOFCharacters_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('Data/cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
